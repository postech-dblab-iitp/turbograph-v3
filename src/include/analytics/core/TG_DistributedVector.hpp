#pragma once
#ifndef TG_DISTRIBUTEDVECTOR_H
#define TG_DISTRIBUTEDVECTOR_H

/*
 * Design of the TG_DistributedVector
 *
 * This class implements a vector that is distributed across multiple machines in a 
 * distributed environment. Specifically, among the components of the distributed 
 * vector, global gather buffer and various functions for the global gather buffer 
 * are implemented.
 *
 * The global gather buffer is a buffer for accumulating updates (transferred in the 
 * form of message). For each update, if the update can be gathered in memory, we 
 * accumulate it to the global gather buffer. If the update cannot be gathered in 
 * memory, it is spilled to disk. Note that this gathers messages generated by all 
 * scatters across machines.
 *
 * The global gather buffer is managed via VersionedArray class for the purpose of 
 * incremental processing. For efficient incremental computations, our dynamic graph
 * store maintains the values at each time point and superstep of global gather buffer
 * in the form of delta. When processing the next time point, these deltas are merged 
 * to restore the global gather buffer without performing the scatter operation again. 
 * Specifically, while processing a superstep at a specific point in time (let's say 
 * time t+1, superstep s), it constructs the global gather buffer at time t, superstep 
 * s+1 asynchronously (Please refer CallConstructNextSuperStepArrayAsync function).
 */

#include "TG_DistributedVectorBase.hpp"
#include "TG_DistributedVectorWindow.hpp"

#include "Turbo_bin_io_handler.hpp"
#include "MemoryMappedArray.hpp"
#include "VersionedArray.hpp"
#include "GBVersionedArray.hpp"
#include "VersionedArrayDiff.hpp"

#include "TG_NWSM.hpp"


#define NO_PER_THREAD_BUFFER 1
#define PER_THREAD_GGB_PULL

class TG_NWSM_Base;

/**
 * Class definition for TG_DistributedVector. The class is a template class
 * of type T and op, where T is the type of vector element and Op is the aggregation
 * operation.
 */
template <typename T, Op op = UNDEFINED>
class TG_DistributedVector: public TG_DistributedVectorBase {
  public:

      /**
       * Class constructor
       */
      TG_DistributedVector() {
          iden_elem = idenelem<T>(op);
          identity_element = idenelem<T>(op);
          vector_exists = false;
          no_logging = false;
          skip_iv = false;
          pull_flag = false;

          alloc_num_entries = -1;
      }
      
      /**
       * Class destructor
       */
      ~TG_DistributedVector() {}

      /**
       * Open vector
       */
      void Open(RDWRMODE Seqmode, RDWRMODE Randmode, int max_lv, std::string VectorName = "", bool _use_diff_semantics=false, bool _no_logging = false, bool _persistent = false, bool _create_read_only = true, bool _use_main_in_versioned_iv = false, bool _flush_ov = true) {
          
          /* Records information of the vector */
          printProcessMemoryUsage("Before Vector Open");
	  max_level = max_lv;
          no_logging = _no_logging;
          UserArguments::CURRENT_LEVEL = 0;
          create_read_only = (create_read_only ? true : _create_read_only);
          use_main_in_versioned_iv = _use_main_in_versioned_iv;
          flush_ov = _flush_ov;
          construct_next_ss_array_async = true;
          use_diff_semantics = _use_diff_semantics;

          Op vec_operation = op;

          rand_write_combined = 1; // XXX - syko
          cur_chunk_merged = -1;
          rand_read_recv_eom_count.store(0);
          rand_read_send_eom_count.store(0);
          rand_write_recv_eom_count.store(0);
          rand_write_send_eom_count.store(0);
          small_message_idx = 0;

          T dummy = 0;
          //datatype = PrimitiveType2MPIType<T>(dummy); //XXX never used
          vectorID = vectorIDtable.size();
          vectorIDtable.push_back(this);

          SeqRDWR = Seqmode;
          RandRDWR = Randmode;

          vector_persistent = _persistent;
          /* Set vectory directory and check if vector file already exists */
          vector_exists = false;
          if (VectorName == "") {
              vector_name = "Vector" + std::to_string(vectorID);
              vector_dir_path = UserArguments::WORKSPACE_PATH + "/" + vector_name;
          } else {
              vector_name = VectorName;
              vector_dir_path = UserArguments::WORKSPACE_PATH + "/" + vector_name;
              if (check_file_exists(vector_dir_path)) {
                  vector_exists = true;
              }
          }

          ov_spilled_updates_path = vector_dir_path + "/ov_tmp";
          ov_path = vector_dir_path + "/ov";
          iv_path = vector_dir_path + "/iv";

          if (!vector_exists) {
              DeleteFilesInDirectory(vector_dir_path, true);
              DeleteFile(vector_dir_path);
              CreateDirectory(vector_dir_path);
              //std::string temp_file_path = "/mnt/tmpfs/" + vector_dir_path;
              //CreateDirectory(temp_file_path);
              //CreateDirectory(vector_dir_path);
          }
          
          /* Select which output vector version array to use */
#ifdef GB_VERSIONED_ARRAY
          if (use_diff_semantics) {
              LOG_ASSERT(false);
              //versioned_ov_array = new VersionedArrayDiff<T, op>();
          //} else versioned_ov_array = new VersionedArray<T, op>();
          } else versioned_ov_array = new GBVersionedArray<T, op>();
          versioned_ov_array->Init(ov_path, PartitionStatistics::my_num_internal_nodes(), false, false, true);
#else
          if (use_diff_semantics) versioned_ov_array = new VersionedArrayDiff<T, op>();
          else versioned_ov_array = new VersionedArray<T, op>();
          versioned_ov_array->Init(ov_path, PartitionStatistics::my_num_internal_nodes(), false, false, true);
#endif

          elm_size = sizeof(T);
          operation = vec_operation;
          cur_read_random_vector_buff
              = cut_read_vector
              = cut_write_vector
              = seq_read_vector
              = seq_write_vector
              = global_gather_buffer
              = NULL;

          global_gather_buffer_vid_range.Set(-1, -1);
          turbo_tcp::establish_all_connections(&server_sockets, &client_sockets, false);
          
          //win.resize(max_level);
          //win = new TG_DistributedVectorWindow<T, op>*[max_level];
          for (int lv = 0; lv < UserArguments::MAX_LEVEL; lv++) {
              //fprintf(stdout, "%s win[%d] = NULL\n", vector_name.c_str(), lv);
              win[lv] = NULL;
          }
          printProcessMemoryUsage("After Vector Open");
      }

      /**
       * Opens window of the vector by calling Open() for given level
       */
      void OpenWindow(RDWRMODE Seqmode, RDWRMODE Randmode, int lv, bool maintain_prev_ss_=false, bool advance_superstep_=true, bool construct_next_ss_array_async_=true) {
          printProcessMemoryUsage("Before VectorWindow Open");
          ALWAYS_ASSERT (win[lv - 1] == NULL);
          //fprintf(stdout, "[%ld] OpenWindow %s, %p, %p, %ld BEGIN\n", PartitionStatistics::my_machine_id(), vector_name.c_str(), this, win[lv - 1], lv -1);
          construct_next_ss_array_async = construct_next_ss_array_async_;
          /* Initialize and open window at level lv */
          win[lv - 1] = new TG_DistributedVectorWindow<T, op>();
          win[lv - 1]->Open(Seqmode, Randmode, this, lv - 1, maintain_prev_ss_, advance_superstep_, construct_next_ss_array_async_);
          printProcessMemoryUsage("After VectorWindow Open");
          //fprintf(stdout, "[%ld] OpenWindow %s, %p, %p, %ld END\n", PartitionStatistics::my_machine_id(), vector_name.c_str(), this, win[lv - 1], lv -1);
      }

      // TODO not using
      void ReleaseOutputVectorReadBuffers() {
          if(RandRDWR == RDONLY || RandRDWR == RDWR) {
              ALWAYS_ASSERT(cur_read_random_vector_buff != NULL);
              ALWAYS_ASSERT(fut_read_random_vector_buff != NULL);
              delete[] cur_read_random_vector_buff;
              delete[] fut_read_random_vector_buff;
              LocalStatistics::register_mem_alloc_info((vector_name + " Double Buffer for Pull").c_str(), 0);
          }
      }
	
    /**
     * Deallocate global gather buffer when closing vector
     */
    void ReleaseGlobalGatherBuffer() {
		if(!(RandRDWR == WRONLY || RandRDWR == RDWR)) return;
        if (global_gather_buffer == NULL) return;
        INVARIANT(OM_global_gather_buffer != NULL);

        /* Close versioned_ov_array */
        versioned_ov_array->Close(true);
        global_gather_buffer = NULL;

        /* Free memory */
        NumaHelper::free_numa_memory(OM_global_gather_buffer, sizeof(T) * global_gather_buffer_vid_range.length());
        OM_global_gather_buffer = NULL;

        global_gather_buffer_vid_range.Set(-1,-1);

        /* Delete all files for spill */
		for (int i = 0; i < UserArguments::VECTOR_PARTITIONS; i++) {
			std::string dir_path = ov_spilled_updates_path + "/" + std::to_string(i);
			for (int j = 0; j < PartitionStatistics::num_target_vector_chunks(); j++) {
                updates_spilling_handler[i][j].Close(true);
                int64_t from_machine_id = j / UserArguments::VECTOR_PARTITIONS;
                int64_t from_chunk_id = j % UserArguments::VECTOR_PARTITIONS;
                std::string msg_path = dir_path + "/" + std::to_string(from_machine_id) + "-" + std::to_string(from_chunk_id);
                DeleteFile(msg_path);
            }
            DeleteFile(dir_path);
		}
		DeleteFile(ov_spilled_updates_path);
    }

    /**
     * Flush all output vector for version u, superstep s
     */
    virtual void FlushOv(int u, int s) {
		if(!(RandRDWR == WRONLY || RandRDWR == RDWR)) return;
	    if (!UserArguments::INCREMENTAL_PROCESSING) return;
        if (no_logging) return;
        
        Range<node_t> vid_range = PartitionStatistics::per_edge_partition_vid_range(PartitionStatistics::my_machine_id() * UserArguments::VECTOR_PARTITIONS + 0);
        node_t my_first_vid = PartitionStatistics::my_first_internal_vid();
        Range<int64_t> idx_range(vid_range.GetBegin() - my_first_vid, vid_range.GetEnd() - my_first_vid);
        
        syko_timer.start_timer(4);

        /* Flush output vector */
        if (flush_ov) versioned_ov_array->FlushAll(u, s);

        syko_timer.stop_timer(4);
    }


    /**
     * Read global gather buffer for version u, superstep s
     */
    virtual void ReadGGB(int u, int s) {
        if (!UserArguments::INCREMENTAL_PROCESSING) return;
        if(!(RandRDWR == WRONLY || RandRDWR == RDWR)) return;
        if (no_logging) return;

        // ReadAndMerge
        Range<node_t> vid_range = PartitionStatistics::per_edge_partition_vid_range(PartitionStatistics::my_machine_id() * UserArguments::VECTOR_PARTITIONS + 0);
        node_t my_first_vid = PartitionStatistics::my_first_internal_vid();
        Range<int64_t> idx_range(vid_range.GetBegin() - my_first_vid, vid_range.GetEnd() - my_first_vid);

        /* Construct buffer at versioned_ov_array */
        syko_timer.start_timer(5);
        versioned_ov_array->ConstructAndFlush(idx_range, u, s, false, NULL, false);
#ifdef GB_VERSIONED_ARRAY
        global_gather_buffer = versioned_ov_array->GetBuffer();
        fprintf(stdout, "%s set global_gather_buffer data() %p\n", vector_name.c_str(), versioned_ov_array->GetBuffer());
#endif
        syko_timer.stop_timer(5);
    }
    
    /**
     * Create input vector for next update (version index next_u)
     */
    virtual void CreateIvForNextUpdate(int next_u) {
	    if (!UserArguments::INCREMENTAL_PROCESSING) return;
        if (no_logging) return;
        if (skip_iv) return;
        for (int lv = 0; lv < UserArguments::MAX_LEVEL; lv++) {
            /* Create input vector for next update version */
        //for (int lv = 0; lv < max_level; lv++) {
            if (win[lv] == NULL) continue;
            win[lv]->CreateIvForNextUpdate(next_u);
        }
    }

    /**
     * Create output vector for next update (version index next_u)
     */
    virtual void CreateOvForNextUpdate(int next_u) {
	    if (!UserArguments::INCREMENTAL_PROCESSING) return;
        if(!(RandRDWR == WRONLY || RandRDWR == RDWR)) return;
        if (no_logging) return;
        if (versioned_ov_array->GetNumberOfUpdateVersions() <= next_u) {
            /* Finish remaining asynchronous job and advance update */
            versioned_ov_array->FinishAsyncJob();
            versioned_ov_array->AdvanceUpdateVersion(next_u);
        }
    }

    /**
     * Create input vector for next superstep next_ss
     */
    virtual void CreateIvForNextSuperstep(int next_ss) {
	    if (!UserArguments::INCREMENTAL_PROCESSING) return;
        if (no_logging) return;
        for (int lv = 0; lv < UserArguments::MAX_LEVEL; lv++) {
        //for (int lv = 0; lv < max_level; lv++) {
            /* Call for each windows */
			if (win[lv] == NULL) continue;
			win[lv]->CreateIvForNextSuperstep(next_ss);
		}
    }
    
    /**
     * Create output vector for next superstep next_ss
     */
    virtual void CreateOvForNextSuperstep(int next_ss) {
	    if (!UserArguments::INCREMENTAL_PROCESSING) return;
        if(!(RandRDWR == WRONLY || RandRDWR == RDWR)) return;
        if (no_logging) return;
        if (versioned_ov_array->GetNumberOfSuperstepVersions(UserArguments::UPDATE_VERSION) <= next_ss) {
            /* Update versioned_ov_array */
            versioned_ov_array->AdvanceSuperstepVersion(UserArguments::UPDATE_VERSION);
#ifdef GB_VERSIONED_ARRAY
            global_gather_buffer = versioned_ov_array->GetBuffer();
            //fprintf(stdout, "%s set global_gather_buffer data() %p\n", vector_name.c_str(), versioned_ov_array->GetBuffer());
#endif
        }
    }
	
    /**
     * Create files of the current superstep
     */
    virtual void CreateFiles(int ss) {
        /* The operation is valid only when the output vector is writeable */
        if(RandRDWR == WRONLY || RandRDWR == RDWR) {
            // Create file handlers for spilled vector writes
			// (ROOT)/(vector_name)/ov/(toChunkID == i)/(fromMachine)-(fromChunkID)
			ALWAYS_ASSERT (UserArguments::VECTOR_PARTITIONS > 0);
			ALWAYS_ASSERT (PartitionStatistics::num_target_vector_chunks() > 0);

            /* Path for spilled updates for output vector */
			CreateDirectory(ov_spilled_updates_path);
			updates_spilling_handler.resize(UserArguments::VECTOR_PARTITIONS);
			//ALWAYS_ASSERT (updates_spilling_handler != NULL);

            /* Manage spilled updates file for each (machine#, chunk#) */
			for (int i = 0; i < UserArguments::VECTOR_PARTITIONS; i++) {
				updates_spilling_handler[i].resize(PartitionStatistics::num_target_vector_chunks());
				//ALWAYS_ASSERT (updates_spilling_handler[i] != NULL);
				std::string dir_path = ov_spilled_updates_path + "/" + std::to_string(i);
				CreateDirectory(dir_path);
				for (int j = 0; j < PartitionStatistics::num_target_vector_chunks(); j++) {
					int64_t from_machine_id = j / UserArguments::VECTOR_PARTITIONS;
					int64_t from_chunk_id = j % UserArguments::VECTOR_PARTITIONS;
					std::string msg_path = dir_path + "/" + std::to_string(from_machine_id) + "-" + std::to_string(from_chunk_id);
                    /* Open file for spilling updates */
					updates_spilling_handler[i][j].OpenFile(msg_path.c_str(), true, true, true);
					ALWAYS_ASSERT(updates_spilling_handler[i][j].file_size() == 0);
				}
			}
		}
    }

    /**
     * Allocate memory of the vector
     */
	virtual void AllocateMemory() {
		if (alloc_num_entries != -1) return;
        
        // Max # of entries in a Vchunk
		max_entry = (PartitionStatistics::max_internal_nodes() + UserArguments::VECTOR_PARTITIONS - 1) / UserArguments::VECTOR_PARTITIONS + UserArguments::VECTOR_PARTITIONS;
		alloc_num_entries = max_entry;

        // Create files for disk spill
        CreateFiles(UserArguments::SUPERSTEP);
		cur_src_vector_vid_range = PartitionStatistics::per_machine_vid_range(PartitionStatistics::my_machine_id());

        // TODO not used 
		small_message_to_send = new WritebackMessage<T>[PER_THREAD_BUFF_SIZE / sizeof(WritebackMessage<T>) * UserArguments::NUM_THREADS];
		if (posix_memalign( (void**) &per_thread_idx, 64, sizeof(PaddedIdx) * UserArguments::NUM_THREADS) != 0) {
			ALWAYS_ASSERT (false);
			abort();
		};

        /* Allocate per-thread write vector */
		per_thread_write_vector = new WritebackMessage<T>*[UserArguments::NUM_THREADS];
		#pragma omp parallel num_threads(UserArguments::NUM_THREADS)
		{
			int i = omp_get_thread_num();
			per_thread_write_vector[i] = (WritebackMessage<T>*) NumaHelper::alloc_numa_local_memory(sizeof(WritebackMessage<T>) * (PER_THREAD_BUFF_SIZE / sizeof(WritebackMessage<T>)));
			ALWAYS_ASSERT (per_thread_write_vector[i] != nullptr);
			memset(per_thread_write_vector[i], 0, sizeof(WritebackMessage<T>) * (PER_THREAD_BUFF_SIZE / sizeof(WritebackMessage<T>)));
			per_thread_idx[i].idx = 0;
		}


		TG_DistributedVectorBase::lock.lock();
		if(TG_DistributedVectorBase::write_vec_changed == nullptr) {
			TG_DistributedVectorBase::write_vec_changed = new node_t[PER_THREAD_BUFF_SIZE / sizeof(node_t)];
			TG_DistributedVectorBase::write_vec_changed_idx.store(0L);
			TG_DistributedVectorBase::update_delta_buffer_overflow.store(false);
		}
		if(TG_DistributedVectorBase::ggb_pull_idx_per_machine == nullptr) {
			TG_DistributedVectorBase::ggb_pull_idx_per_machine = new node_t*[PartitionStatistics::num_machines()];
			if (posix_memalign( (void**) &TG_DistributedVectorBase::ggb_pull_idx_per_machine_idx, 64, sizeof(PaddedIdx) * PartitionStatistics::num_machines()) != 0) {
				ALWAYS_ASSERT (false);
				abort();
			};
			TG_DistributedVectorBase::ggb_pull_idx_overflow = new bool[PartitionStatistics::num_machines()];
			for (int64_t i = 0; i < PartitionStatistics::num_machines(); i++) {
				TG_DistributedVectorBase::ggb_pull_idx_per_machine[i] = new node_t[PER_THREAD_BUFF_SIZE / sizeof(node_t)];
				TG_DistributedVectorBase::ggb_pull_idx_per_machine_idx[i].idx = 0;
				TG_DistributedVectorBase::ggb_pull_idx_overflow[i] = false;
			}
		}
		TG_DistributedVectorBase::lock.unlock();

        /* Allocate memory for window */
		for (int lv = 0; lv < UserArguments::MAX_LEVEL; lv++) {
		//for (int lv = 0; lv < max_level; lv++) {
            /* Allocate memory for each window */
			if (win[lv] == NULL) continue;
			win[lv]->AllocateMemory(create_read_only);
		}
		int64_t size_once = DEFAULT_NIO_BUFFER_SIZE / sizeof(WritebackMessage<T>);
	}
    
    /**
     * Set to construct vector of previous snapshot 
     */
    virtual void OpenReadOnlyPrev() {
        create_read_only = true;
    }
    
    /**
     * Set not to construct vector of previous snapshot 
     */
    virtual void CloseReadOnlyPrev() {
        create_read_only = false;
    }
    
    /**
     * Returns whether the vector maintains vector of previous snapshot
     */
    virtual bool IsReadOnlyPrevOpened() {
        return create_read_only;
    }
    
    /**
     * Set input vector to construct next super array asynchronously
     */
    virtual void SetConstructNextSuperStepArrayAsync(bool construct_next_ss_array_async_) {
        win[0]->SetConstructNextSuperStepArrayAsync(construct_next_ss_array_async_);
    }
    
    /**
     * Set whether the version is constructed already
     */
    virtual void SetConstructedAlready(bool constructed_already) {
        win[0]->SetConstructedAlready(constructed_already);
    }

    /**
     * Return whether concurrent superstep construction is turned on
     */
    virtual bool CheckConstructNextSuperStepArrayAsync() {
        return construct_next_ss_array_async;
    }
    
    /**
     * Set input vector to advance superstep
     */
    virtual void SetAdvanceSuperstep(bool advance_superstep_) {
        win[0]->SetAdvanceSuperstep(advance_superstep_);
    }
	
    /**
     * Wait for vector IO to be finished
     */
    virtual void WaitForVectorIO() {
        ALWAYS_ASSERT (UserArguments::SCHEDULE_TYPE != -1);

        syko_timer.start_timer(6);
        for (int lv = 0; lv < UserArguments::MAX_LEVEL; lv++) {
        //for (int lv = 0; lv < max_level; lv++) {
            if (win[lv] == NULL) continue;
            win[lv]->WaitForVectorIO();
        }
        syko_timer.stop_timer(6);

        if ((CheckTargetForPull() && UserArguments::USE_PULL) || (!UserArguments::USE_PULL && (RandRDWR == RDWR || RandRDWR == WRONLY))) {
            int64_t sum = 0;
            int64_t delta = 1;
            int64_t mul = (PartitionStatistics::num_target_vector_chunks() * UserArguments::VECTOR_PARTITIONS);

            for (int i = 0; i < UserArguments::MAX_LEVEL; i++) {
            //for (int i = 0; i < max_level; i++) {
                if (i > 0) delta = delta * PartitionStatistics::num_target_vector_chunks();
                if (win[i] == NULL) continue;
                sum += mul * delta;
            }

            /* Wait for output vector receive */
            syko_timer.start_timer(7);
            while (rand_write_recv_eom_count.load() != sum) {
                ALWAYS_ASSERT(rand_write_recv_eom_count.load() <= sum);
                _mm_pause();
            }
            syko_timer.stop_timer(7);
            
            /* Wait for output vector send */
            syko_timer.start_timer(8);
            while (rand_write_send_eom_count.load() != sum) {
                ALWAYS_ASSERT(rand_write_send_eom_count.load() <= sum);
                _mm_pause();
            }
            syko_timer.stop_timer(8);

            rand_write_recv_eom_count -= sum;
            rand_write_send_eom_count -= sum;

            INVARIANT(rand_write_send_eom_count.load() == 0L);
            INVARIANT(rand_write_recv_eom_count.load() == 0L);
        }

        /* With single-machine execution environment,
         * we need special post-processing */
#ifdef TEMP_SINGLE_MACHINE
        if (PartitionStatistics::num_machines() == 1)
            PostprocessingAfterVectorIO();
#endif
    }

    /**
     * Post processing after vector IO in single-machine environment
     */
    void PostprocessingAfterVectorIO() {
		if(!(RandRDWR == WRONLY || RandRDWR == RDWR)) return;
        INVARIANT(PartitionStatistics::num_machines() == 1);

        // For INC_STEP 2,
        // Mark versioned array ov to flush, Mark Reapply flag
        // TODO why inc_step == -1??
        if (UserArguments::INC_STEP == -1) {
            TG_DistributedVectorBase::ggb_msg_received_flags.InvokeIfMarked([&](int64_t idx) {
                versioned_ov_array->MarkChanged(idx);
#ifndef GB_VERSIONED_ARRAY
                versioned_ov_array->MarkToFlush(idx);
#endif
            });
        } else if ((UserArguments::INC_STEP == 3) && UserArguments::USE_PULL) {
            /*TG_DistributedVectorBase::ggb_pull_idx_overflow[0] = true;
            ggb_pull_message_store_flags[0].CopyFrom(TG_DistributedVectorBase::ggb_msg_received_flags
            if (TG_DistributedVectorBase::ggb_msg
            TG_DistributedVectorBase::ggb_msg_received_flags.InvokeIfMarked([&](int64_t idx) {
                int64_t tmp_idx = std::atomic_fetch_add((std::atomic<int64_t>*) &TG_DistributedVectorBase::ggb_pull_idx_per_machine_idx[partition_id].idx, +1L);
                if (tmp_idx < PER_THREAD_BUFF_SIZE / sizeof(node_t)) {
                    TG_DistributedVectorBase::ggb_pull_idx_per_machine[partition_id][tmp_idx] = idx;
                } else {
                    TG_DistributedVectorBase::ggb_pull_idx_overflow[partition_id] = true;
                    ggb_pull_message_store_flags_per_machine[partition_id].TwoLevelBitMap<int64_t>::Set_Atomic_IfNotSet(loc);
                }
            });*/
        }
        /*else if (UserArguments::INC_STEP == 2) {
            if (!use_diff_semantics) return; //XXX for this case how to process?
            T* ov_buffer = versioned_ov_array->GetBuffer();
            T* ov_buffer_prev = versioned_ov_array->GetPrevBuffer();
            TG_DistributedVectorBase::ggb_msg_received_flags.InvokeIfMarked([&](int64_t idx) {
                if (!checkEquality(ov_buffer[idx], ov_buffer_prev[idx])) {
                    versioned_ov_array->MarkToFlush(idx);
                    MarkReapplyRequiredByIdx(idx);
                }
            });
        }*/
    }

    /**
     * Close vector
     */
	virtual void Close() {
        /* Assure all remaining outputvector writes are finished */
		INVARIANT(rand_write_send_eom_count.load() == 0);
		INVARIANT(rand_write_recv_eom_count.load() == 0);
		
        //fprintf(stdout, "[%ld] Close %s, %p\n", PartitionStatistics::my_machine_id(), vector_name.c_str(), this);
        
        //for (int lv = 0; lv < max_level; lv++) {
        /**
         * Close all window
         */
        for (int lv = 0; lv < UserArguments::MAX_LEVEL; lv++) {
			if (win[lv] != NULL) {
				win[lv]->Close();
				delete win[lv];
				win[lv] = NULL;
            }
        }
        //win.clear();
        /*
        if (win != NULL) {
            delete win;
            win = NULL;
        }
        */

       /* Free all related memory regions */
        ReleaseGlobalGatherBuffer();
		if (!vector_persistent) {
			//DeleteFilesInDirectory(vector_dir_path, true);
            //DeleteFile(vector_dir_path);
        }

		for(int64_t i = 0 ; i < UserArguments::NUM_THREADS ; i++) {
			NumaHelper::free_numa_local_memory(per_thread_write_vector[i], sizeof(WritebackMessage<T>) * (PER_THREAD_BUFF_SIZE / sizeof(WritebackMessage<T>)));
		}
		delete[] per_thread_write_vector;
		delete per_thread_idx;
		delete[] small_message_to_send;
	}

    /**
     * Request output vector pull
     */
	void RequestOutputVectorPull(int partition_id, int64_t chunkID) {
        /* Calculate amount of entry to receive */
		int64_t entry_to_recv = PartitionStatistics::machine_id_and_chunk_idx_to_vid_range(partition_id, chunkID).length();
		int64_t buff_loc = 0;
		tslee_timer.start_timer(2);
		turbo_timer tmr;
		tmr.start_timer(0);
		while(true) {
			ALWAYS_ASSERT(entry_to_recv >= 0);
            /* If entries to receive is more than the size of single buffer, receive
             * multiple times. Otherwise, just receieve one time */
			if(entry_to_recv > DEFAULT_NIO_BUFFER_SIZE / sizeof(T)) {
				client_sockets.recv_from_server((char*)&fut_read_random_vector_buff[0], buff_loc * sizeof(T), partition_id);
				buff_loc += DEFAULT_NIO_BUFFER_SIZE / sizeof(T);
				entry_to_recv -= DEFAULT_NIO_BUFFER_SIZE / sizeof(T);
			} else {
				client_sockets.recv_from_server((char*)&fut_read_random_vector_buff[0], buff_loc * sizeof(T), partition_id);
				break;
			}
		}
		tslee_timer.stop_timer(2);
		tmr.stop_timer(0);
	}

    /**
     * Caller function for RequestOutputVectorPull()
     */ 
	static void callRequestOutputVectorPull(TG_DistributedVector<T, op>* vec, int partition_id, int64_t chunkID) {
		vec->RequestOutputVectorPull(partition_id, chunkID);
	}

	//To Respond Request
    // TODO not using
	virtual void RespondSequentialVectorPull(int64_t chunkID, int partition_id, int lv) {
	    LOG_ASSERT(false);
    }

    /**
     * Send input vector pull
     */
    static void SendInputVectorPull(tbb::concurrent_queue<UdfSendRequest>* req_data_queue, int64_t chunkdID, int partition_id, int lv, turbo_tcp* server_sockets_) {
        UdfSendRequest udf_req;
        SimpleContainer cont;
        UserCallbackRequest req;

        while (true) {
            /* Implementation of backoff policy when req_data_queue is not ready */
            int backoff = 1;
            while (!req_data_queue->try_pop(udf_req)) {
                usleep (backoff * 4);
                backoff = (backoff >= 1024) ? 1024 : 2 * backoff;
            }

            cont = udf_req.cont;
            req = udf_req.req;

            /* Stop send when the request is empty */
            if (cont.data == NULL && cont.capacity == 0 && cont.size_used == 0) {
                break;
            }
            
            /* Send the request */
            server_sockets_->send_to_client((char*) cont.data, 0, req.data_size, partition_id);
            RequestRespond::ReturnTempDataBuffer(cont);
        }
        /* Send end of message */
        char eom;
        server_sockets_->send_to_client((char*) &eom, 0, 0, partition_id);
    }

    /**
     * Respond input vector pull
     */
	virtual void RespondInputVectorPull(int64_t chunkID, int partition_id, int lv) {
        //if (pull_flag) RespondInputVectorPull_Opt(chunkID, partition_id, lv);
        //else RespondInputVectorPull_Orig(chunkID, partition_id, lv);
        turbo_timer respond_timer;
        tbb::concurrent_queue<UdfSendRequest>* req_data_queue = new tbb::concurrent_queue<UdfSendRequest>();
        std::future<void> wait_on_sender_thread = Aio_Helper::async_pool.enqueue_hot(TG_DistributedVector::SendInputVectorPull, req_data_queue, chunkID, partition_id, lv, &server_sockets);
        respond_timer.start_timer(0);
        RespondInputVectorPull_Orig(req_data_queue, chunkID, partition_id, lv);
        respond_timer.stop_timer(0);
        respond_timer.start_timer(1);
        wait_on_sender_thread.wait();
        respond_timer.stop_timer(1);
        rand_write_send_eom_count += 1; // syko
        delete req_data_queue;
        //fprintf(stdout, "DONE [%ld] (%ld,%ld) [RespondInputVectorPull] send to %ld / %.4f %.4f\n", PartitionStatistics::my_machine_id(), UserArguments::UPDATE_VERSION, UserArguments::SUPERSTEP, partition_id, respond_timer.get_timer(0), respond_timer.get_timer(1));
    }
	
    /**
     * Respond to input vector pull (optimized)
     */
    void RespondInputVectorPull_Opt(int64_t chunkID, int partition_id, int lv) {
        INVARIANT(false);
		int64_t size_once = DEFAULT_NIO_BUFFER_SIZE / sizeof(WritebackMessage<T>);
        int64_t size_to_send = size_once * sizeof(WritebackMessage<T>);
        //fprintf(stdout, "[%ld] (%ld,%ld) [RespondInputVectorPull_Opt] send to %ld; pull_buffer_idx = %ld lgb_toggle = %ld\n", PartitionStatistics::my_machine_id(), UserArguments::UPDATE_VERSION, UserArguments::SUPERSTEP, partition_id, pull_buffer_idx, TG_DistributedVectorBase::lgb_toggle);
		turbo_timer tmr;
		tmr.start_timer(0);

		int64_t entry_to_send = pull_buffer_idx;
		int64_t accm_sent_bytes = 0;
        while (true) {
            if (entry_to_send == 0) break;
            if (entry_to_send > size_once) {
                tmr.start_timer(1);
                server_sockets.send_to_client((char*) OM_global_gather_buffer, accm_sent_bytes, size_to_send, partition_id);
                tmr.stop_timer(1);
                accm_sent_bytes += size_to_send;
				entry_to_send -= size_once;
            } else {
                tmr.start_timer(1);
                server_sockets.send_to_client((char*) OM_global_gather_buffer, accm_sent_bytes, entry_to_send * sizeof(WritebackMessage<T>), partition_id);
                tmr.stop_timer(1);
                break;
            }
        }
            
        tmr.start_timer(1);
        server_sockets.send_to_client((char*) OM_global_gather_buffer, 0, 0, partition_id);
        tmr.stop_timer(1);
		tmr.stop_timer(0);
        
#ifdef REPORT_PROFILING_TIMERS
        fprintf(stdout, "DONE [%ld] (%ld,%ld) [RespondInputVectorPull] send to %ld / %.2f %.2f / %.2f MB/sec\n", PartitionStatistics::my_machine_id(), UserArguments::UPDATE_VERSION, UserArguments::SUPERSTEP, partition_id, tmr.get_timer(0), tmr.get_timer(1), (double) accm_sent_bytes / (1024 * 1024 * tmr.get_timer(1)));
#endif
        rand_write_send_eom_count += 1; // syko
    }

    /**
     * Respond to input vector pull
     */
	void RespondInputVectorPull_Orig(tbb::concurrent_queue<UdfSendRequest>* req_data_queue, int64_t chunkID, int partition_id, int lv) {
        /* The behavior depends on ggb overflow */
        bool overflow = TG_DistributedVectorBase::ggb_pull_idx_overflow[partition_id];
        if (!overflow) {
            /** Case when overflow did not happen */
#ifdef OldMessageTransfer
            int64_t size_once = DEFAULT_NIO_BUFFER_SIZE / sizeof(WritebackMessageWOOldMessage<T>);
            WritebackMessageWOOldMessage<T>* send_buff_;
#else
            LOG_ASSERT(false);
#endif
            SimpleContainer cont = RequestRespond::GetTempDataBuffer(DEFAULT_NIO_BUFFER_SIZE);
            send_buff_ = (WritebackMessageWOOldMessage<T>*) cont.data;
            int64_t send_buff_idx = 0;
            
            T* vector_for_pull = win[lv]->pull_send_buff;
            if (vector_for_pull == NULL) {
                fprintf(stdout, "%s vector_for_pull NULL\n", this->vector_name.c_str());
            }

            ALWAYS_ASSERT (vector_for_pull != NULL);
            for (int64_t i = 0; i < TG_DistributedVectorBase::ggb_pull_idx_per_machine_idx[partition_id].idx; i++) {
                send_buff_[send_buff_idx].message = vector_for_pull[TG_DistributedVectorBase::ggb_pull_idx_per_machine[partition_id][i]];
                send_buff_[send_buff_idx].dst_vid = TG_DistributedVectorBase::ggb_pull_idx_per_machine[partition_id][i] + PartitionStatistics::my_first_node_id();
                send_buff_idx++;
                
                if (send_buff_idx == size_once) {
                    UdfSendRequest udf_req;
                    udf_req.cont = cont;
                    udf_req.req.data_size = size_once * sizeof(WritebackMessageWOOldMessage<T>);
                    req_data_queue->push(udf_req);
                    cont = RequestRespond::GetTempDataBuffer(DEFAULT_NIO_BUFFER_SIZE);
                    send_buff_ = (WritebackMessageWOOldMessage<T>*) cont.data;
                    send_buff_idx = 0;
                }
            }
            
            if (send_buff_idx > 0) {
                UdfSendRequest udf_req;
                udf_req.cont = cont;
                udf_req.req.data_size = send_buff_idx * sizeof(WritebackMessageWOOldMessage<T>);
                req_data_queue->push(udf_req);
                send_buff_idx = 0;
            } else {
                RequestRespond::ReturnTempDataBuffer(cont);
            }

            UdfSendRequest udf_req_eom;
            SimpleContainer cont_eom;
            cont_eom.data = nullptr;
            cont_eom.size_used = 0;
            cont_eom.capacity = 0;
            udf_req_eom.cont = cont_eom;
            req_data_queue->push(udf_req_eom);

        } else {
            /* Case when overflow occured */
            //for (int64_t i = 0; i < TG_DistributedVectorBase::ggb_pull_idx_per_machine_idx[partition_id].idx; i++) {
            for (int64_t i = 0; i < PER_THREAD_BUFF_SIZE / sizeof(node_t); i++) {
                TG_DistributedVectorBase::ggb_pull_message_store_flags_per_machine[partition_id].Set(TG_DistributedVectorBase::ggb_pull_idx_per_machine[partition_id][i]);
            }
#ifdef OldMessageTransfer
            int64_t size_once = DEFAULT_NIO_BUFFER_SIZE / sizeof(WritebackMessageWOOldMessage<T>);
            WritebackMessageWOOldMessage<T>* send_buff_[UserArguments::NUM_THREADS];
            SimpleContainer cont_idx = RequestRespond::GetTempDataBuffer(DEFAULT_NIO_BUFFER_SIZE);
            PaddedIdx* idx_ = (PaddedIdx*) cont_idx.data;
            //SimpleContainer cont_cnt = RequestRespond::GetTempDataBuffer(DEFAULT_NIO_BUFFER_SIZE);
            //PaddedIdx* per_thread_cnt_ = (PaddedIdx*) cont_cnt.data;
#else
            int64_t size_once = DEFAULT_NIO_BUFFER_SIZE / sizeof(WritebackMessage<T>);
            WritebackMessage<T>* send_buff_[UserArguments::NUM_THREADS];
            int64_t idx_[UserArguments::NUM_THREADS];
#endif
            SimpleContainer cont[UserArguments::NUM_THREADS];
            //#pragma omp parallel num_threads(UserArguments::NUM_THREADS)
            for (int i = 0; i < UserArguments::NUM_THREADS; i++) {
                idx_[i].idx = 0;
                //per_thread_cnt_[i].idx = 0;
                cont[i] = RequestRespond::GetTempDataBuffer(DEFAULT_NIO_BUFFER_SIZE);
                ALWAYS_ASSERT (cont[i].capacity == DEFAULT_NIO_BUFFER_SIZE);
#ifdef OldMessageTransfer
                send_buff_[i] = (WritebackMessageWOOldMessage<T>*) cont[i].data;
#else
                send_buff_[i] = (WritebackMessage<T>*) cont[i].data;
#endif
            }

            T* vector_for_pull = win[lv]->pull_send_buff;
            if (vector_for_pull == NULL) {
                fprintf(stdout, "%s vector_for_pull NULL\n", this->vector_name.c_str());
            }
            ALWAYS_ASSERT (vector_for_pull != NULL);

            //fprintf(stdout, "[%ld] (%ld,%ld) [RespondInputVectorPull_Orig] send to %ld; lgb_toggle = %ld ggb_total = %ld\n", PartitionStatistics::my_machine_id(), UserArguments::UPDATE_VERSION, UserArguments::SUPERSTEP, partition_id, TG_DistributedVectorBase::lgb_toggle, ggb_pull_message_store_flags.GetTotal());

            // XXX - Optimization: Parallelize it
            // - Pack the messages in OM_GGB and send
            //int64_t idx__ = 0;
            int64_t accm_sent_bytes = 0;
#ifndef PER_THREAD_GGB_PULL
            LOG_ASSERT(false);
            /*#else
              int64_t num_total = ggb_pull_message_store_flags.GetTotal();
              if (num_total != 0) {
              ggb_pull_message_store_flags.TwoLevelBitMap<int64_t>::InvokeIfMarked([&](node_t idx) {
#endif*/
#endif
            //int64_t num_total = ggb_pull_message_store_flags_per_machine[partition_id].GetTotal();
            ggb_pull_message_store_flags_per_machine[partition_id].TwoLevelBitMap<int64_t>::InvokeIfMarked([&](node_t idx) {
                int i = omp_get_thread_num();
                node_t vid = idx + PartitionStatistics::my_first_node_id();
                ALWAYS_ASSERT(vid >= PartitionStatistics::my_first_node_id() && vid <= PartitionStatistics::my_last_node_id());
                send_buff_[i][idx_[i].idx].message = vector_for_pull[idx];
                send_buff_[i][idx_[i].idx].dst_vid = vid;

                ALWAYS_ASSERT(PartitionStatistics::my_first_node_id() <= send_buff_[i][idx_[i].idx].dst_vid);
                ALWAYS_ASSERT(PartitionStatistics::my_last_node_id() >= send_buff_[i][idx_[i].idx].dst_vid);
                ALWAYS_ASSERT(PartitionStatistics::my_internal_vid_range().contains(send_buff_[i][idx_[i].idx].dst_vid));

                if (++idx_[i].idx == size_once) {
                    //per_thread_cnt_[i].idx++;
                    UdfSendRequest udf_req;
                    udf_req.cont = cont[i];
                    udf_req.req.data_size = size_once * sizeof(WritebackMessageWOOldMessage<T>);
                    req_data_queue->push(udf_req);
                    cont[i] = RequestRespond::GetTempDataBuffer(DEFAULT_NIO_BUFFER_SIZE);
                    send_buff_[i] = (WritebackMessageWOOldMessage<T>*) cont[i].data;
                    idx_[i].idx = 0;
                }
            }, false);

            for (int i = 0; i < UserArguments::NUM_THREADS; i++) {
                if (idx_[i].idx > 0) {
                    //per_thread_cnt_[i].idx++;
                    UdfSendRequest udf_req;
                    udf_req.cont = cont[i];
                    udf_req.req.data_size = idx_[i].idx * sizeof(WritebackMessageWOOldMessage<T>);
                    req_data_queue->push(udf_req);
                    //idx_[i] = 0;
                } else {
                    RequestRespond::ReturnTempDataBuffer(cont[i]);
                }
            }
            ggb_pull_message_store_flags_per_machine[partition_id].ClearAll();

            UdfSendRequest udf_req_eom;
            SimpleContainer cont_eom;
            cont_eom.data = nullptr;
            cont_eom.size_used = 0;
            cont_eom.capacity = 0;
            udf_req_eom.cont = cont_eom;
            req_data_queue->push(udf_req_eom);

            RequestRespond::ReturnTempDataBuffer(cont_idx);
        }
        
        //TG_DistributedVectorBase::ggb_pull_idx_per_machine_idx[partition_id].idx = 0;
        //TG_DistributedVectorBase::ggb_pull_idx_overflow[partition_id] = false;
        //fprintf(stdout, "DONE [%ld] (%ld,%ld) [RespondInputVectorPull] send to %ld / %.2f %.2f / %.2f MB / %.2f MB/sec\n%s", PartitionStatistics::my_machine_id(), UserArguments::UPDATE_VERSION, UserArguments::SUPERSTEP, partition_id, tmr.get_timer(0), tmr.get_timer(1), (double) accm_sent_bytes / (1024 * 1024), (double) accm_sent_bytes / (1024 * 1024 * tmr.get_timer(1)), to_print.c_str());
#ifdef REPORT_PROFILING_TIMERS
        fprintf(stdout, "DONE [%ld] (%ld,%ld) [RespondInputVectorPull] send to %ld / %.2f %.2f / %.2f MB/sec\n", PartitionStatistics::my_machine_id(), UserArguments::UPDATE_VERSION, UserArguments::SUPERSTEP, partition_id, tmr.get_timer(0), tmr.get_timer(1), (double) accm_sent_bytes / (1024 * 1024 * tmr.get_timer(1)));
#endif
    }

	/**
     * Respond output vector pull
     */
	virtual void RespondOutputVectorPull(int64_t chunkID, int partition_id, int lv) {
		ALWAYS_ASSERT(0 <= chunkID);
		ALWAYS_ASSERT(chunkID < UserArguments::VECTOR_PARTITIONS);

		tslee_timer.start_timer(0);
		tslee_timer.start_timer(1);
		turbo_timer tmr;
		tmr.start_timer(0);
		int64_t entry_to_send = PartitionStatistics::my_chunkID_to_range(chunkID).length() * elm_size;
		int64_t buff_loc = (PartitionStatistics::my_chunkID_to_range(chunkID).GetBegin() - PartitionStatistics::my_first_node_id());
        /* Respond each data in buffer */
		while(true) {
			ALWAYS_ASSERT(entry_to_send >= 0);
            if(entry_to_send > DEFAULT_NIO_BUFFER_SIZE) {
                server_sockets.send_to_client((char*)win[0]->cur_iv_mmap->data(), buff_loc * sizeof(T), DEFAULT_NIO_BUFFER_SIZE, partition_id);

				buff_loc += DEFAULT_NIO_BUFFER_SIZE / sizeof(T);
				entry_to_send -= DEFAULT_NIO_BUFFER_SIZE;
			} else {
                server_sockets.send_to_client((char*)win[0]->cur_iv_mmap->data(), buff_loc * sizeof(T), entry_to_send, partition_id);
				break;
			}
		}
		tslee_timer.stop_timer(0);
		tslee_timer.stop_timer(1);
		tmr.stop_timer(0);
	}
    
    /**
     * Set flag for pull
     */
    virtual void SetFlagForPull(bool flag) {
        //fprintf(stdout, "[%ld] (%d,%d) %s SetFlagForPull %s\n", PartitionStatistics::my_machine_id(), UserArguments::UPDATE_VERSION, UserArguments::SUPERSTEP, this->vector_name.c_str(), flag ? "true" : "false");
        pull_flag = flag;
        pull_buffer_idx = 0;
    }

    /**
     * Respond to output vector push. Internally calls different functions depending on incremental/static mode.
     */
    virtual void RespondOutputVectorMessagePush(int64_t fromChunkID, int64_t chunkID, int partition_id, int tid, int idx, int64_t send_num, int64_t combined, int lv) {
        if (!UserArguments::INCREMENTAL_PROCESSING || UserArguments::INC_STEP == -1 || no_logging) {
            // AcitveVertices, StartingVertices
            RespondOutputVectorMessagePushForStaticProcessing(fromChunkID, chunkID, partition_id, tid, idx, send_num, combined, lv);
        } else {
            RespondOutputVectorMessagePushForIncProcessing(fromChunkID, chunkID, partition_id, tid, idx, send_num, combined, lv);
        }
    }
	
    virtual void RespondOutputVectorMessagePushForStaticProcessing(int64_t fromChunkID, int64_t chunkID, int partition_id, int tid, int idx, int64_t send_num, int64_t combined, int lv);

    /**
     * Accumulate message to GGB in pull mode 
     */
    void PULL_AccumulateMessageIntoGGB(node_t vid, T val) {
        ALWAYS_ASSERT(UserArguments::INC_STEP == 4);
        ALWAYS_ASSERT(IsReaggregationRequired(vid));
        int64_t loc = vid - PartitionStatistics::my_first_node_id();
        MARK_CNT_64BIT(val, 1); // XXX assume 64bit
        //PerformAtomicOperation(&global_gather_buffer[loc], val, loc);
        _AccumulateNewAndOldMessagesIntoGGB(loc, val);
    }
    
    // TODO not using
    void _MergeGgbAndOmGgb(int64_t loc) {
        switch (op) {
            case MAX:
            case MIN:
                break;
            case PLUS:
                //AtomicUpdateAggregation<T, op>(&global_gather_buffer[loc], msg.old_message, msg.message);
                //PerformAtomicOperation(&global_gather_buffer[loc], OM_global_gather_buffer[loc], loc);   // XXX how to deal with PageRank?
                break;
            default:
                LOG_ASSERT(false);
                break;
        }
    }
    
    /**
     * GGB message accumulation behavior definition
     */
#ifdef OldMessageTransfer
    void _AccumulateNewAndOldMessagesIntoGGB(int64_t loc, T& new_msg) {
        switch (op) {
            case MAX:
            case MIN:
                AtomicUpdateAggregationWithCnt<T, op>(&global_gather_buffer[loc], new_msg);
                break;
            case MULTIPLY:
            case PLUS:
                AtomicOperation<T, op>(&global_gather_buffer[loc], new_msg);
                break;
            default:
                AtomicOperation<T, op>(&global_gather_buffer[loc], new_msg);
                //LOG_ASSERT(false);
                break;
        }
    }
    // TODO change the name of OMGGB -> "NMGGB"
    void _AccumulateNewAndOldMessagesIntoOMGGB(int64_t loc, T& new_msg) {
        switch (op) {
            case MAX:
            case MIN:
                AtomicUpdateAggregationWithCnt<T, op>(&OM_global_gather_buffer[loc], new_msg);
                break;
            case MULTIPLY:
            case PLUS:
                AtomicOperation<T, op>(&OM_global_gather_buffer[loc], new_msg);
                break;
            default:
                AtomicOperation<T, op>(&OM_global_gather_buffer[loc], new_msg);
                //LOG_ASSERT(false);
                break;
        }
    }
    void _AccumulateNewAndOldMessages(T* target, T& new_msg) {
        switch (op) {
            case MAX:
            case MIN:
                UpdateAggregationWithCnt<T, op>(target, new_msg);
                break;
            case MULTIPLY:
            case PLUS:
                Operation<T, op>(target, new_msg);
                break;
            default:
                Operation<T, op>(target, new_msg);
                //LOG_ASSERT(false);
                break;
        }
    }
#else
    void _AccumulateNewAndOldMessagesIntoGGB(int64_t loc, T new_msg) {
        INVARIANT(false);
        /*switch (op) {
            case MAX:
            case MIN:
                AtomicOperation<T, op>(&global_gather_buffer[loc], new_msg);
                AtomicOperation<T, op>(&OM_global_gather_buffer[loc], old_msg);
                break;
            case MULTIPLY:
            case PLUS:
                AtomicUpdateAggregation<T, op>(&global_gather_buffer[loc], old_msg, new_msg);
                //AtomicUpdateAggregation<T, op>(&OM_global_gather_buffer[loc], old_msg, new_msg);
                break;
            default:
                LOG_ASSERT(false);
                break;
        }*/
    }
#endif
    
#ifdef OldMessageTransfer
    /**
      * GGB message accumulate definition for old message transfer
      */
    void _AccumulateMessageIntoGGB(WritebackMessageWOOldMessage<T>& msg, int partition_id=-1) {
        ALWAYS_ASSERT (global_gather_buffer_vid_range.contains(msg.dst_vid));
        int64_t loc = msg.dst_vid - global_gather_buffer_vid_range.GetBegin();
				//if (UserArguments::UPDATE_VERSION >= 1)
	      //  fprintf(stdout, "[%ld] (%ld, %ld)\t[%s _AccumulateMessageIntoGGB]\tGGB[%ld] = (%ld, cnt %d) <-- Msg[%ld] = (%ld, cnt %d) loc = %ld @ INC_STEP = %ld\n", PartitionStatistics::my_machine_id(), (int64_t) UserArguments::UPDATE_VERSION, (int64_t) UserArguments::SUPERSTEP, vector_name.c_str(), (int64_t) msg.dst_vid, (int64_t) GET_VAL_64BIT(global_gather_buffer[loc]), (int) GET_CNT_64BIT(global_gather_buffer[loc]), (int64_t) msg.dst_vid, (int64_t) GET_VAL_64BIT(msg.message), (int) GET_CNT_64BIT(msg.message), (int64_t) loc, (int64_t) UserArguments::INC_STEP);
#ifdef INCREMENTAL_LOGGING
        //std::cout << vector_name << " AccumulateMessageIntoGGB(" << msg.dst_vid << "), Old: " << msg.old_message << ", New: " << msg.message << ", GGB: " << global_gather_buffer[loc] << "\n"; 
        if (INCREMENTAL_DEBUGGING_TARGET(msg.dst_vid)) {
            fprintf(stdout, "(%ld, %ld)\t[_AccumulateMessageIntoGGB]\tGGB[%ld] = %s <-- Msg[%ld] = %s, old_msg = %s, loc = %ld @ INC_STEP = %ld\n", (int64_t) UserArguments::UPDATE_VERSION, (int64_t) UserArguments::SUPERSTEP, (int64_t) msg.dst_vid, to_string_with_precision(global_gather_buffer[loc]).c_str(), (int64_t) msg.dst_vid, to_string_with_precision(msg.message).c_str(), to_string_with_precision(msg.old_message).c_str(), (int64_t) loc, (int64_t) UserArguments::INC_STEP);
            //fprintf(stdout, "[%ld] (%ld, %ld)\t[%s _AccumulateMessageIntoGGB]\tGGB[%ld] = (%ld, cnt %d) <-- Msg[%ld] = (%ld, cnt %d) loc = %ld @ INC_STEP = %ld\n", PartitionStatistics::my_machine_id(), (int64_t) UserArguments::UPDATE_VERSION, (int64_t) UserArguments::SUPERSTEP, vector_name.c_str(), (int64_t) msg.dst_vid, (int64_t) GET_VAL_64BIT(global_gather_buffer[loc]), (int) GET_CNT_64BIT(global_gather_buffer[loc]), (int64_t) msg.dst_vid, (int64_t) GET_VAL_64BIT(msg.message), (int) GET_CNT_64BIT(msg.message), (int64_t) loc, (int64_t) UserArguments::INC_STEP);
        }
#endif
        /* Incremental step 1 */
        if (UserArguments::INC_STEP == -1) {
            MarkOvMessageReceived(loc); //XXX need?
            _AccumulateNewAndOldMessagesIntoGGB(loc, msg.message);
            versioned_ov_array->MarkChanged(loc); //annotated originally
#ifndef GB_VERSIONED_ARRAY
            versioned_ov_array->MarkToFlush(loc);
#endif
        /* Incremental step 2 */
        } else if (UserArguments::INC_STEP == 2) {
            MarkOvMessageReceived(loc);
            _AccumulateNewAndOldMessagesIntoOMGGB(loc, msg.message);
#ifdef PER_THREAD_GGB_PULL
    /* Incremental step 3 for pull mode*/
        } else if (UserArguments::INC_STEP == 3 && partition_id != -1) {
            ALWAYS_ASSERT(partition_id >= 0 && partition_id < PartitionStatistics::num_machines());
            int64_t tmp_idx = std::atomic_fetch_add((std::atomic<int64_t>*) &TG_DistributedVectorBase::ggb_pull_idx_per_machine_idx[partition_id].idx, +1L);
            //if (TG_DistributedVectorBase::ggb_pull_idx_per_machine_idx[partition_id] != PER_THREAD_BUFF_SIZE / sizeof(node_t)) {
            if (tmp_idx < PER_THREAD_BUFF_SIZE / sizeof(node_t)) {
                //if (!(TG_DistributedVectorBase::ggb_pull_idx_per_machine_idx[partition_id] < PER_THREAD_BUFF_SIZE / sizeof(node_t))) {
                //	fprintf(stdout, "[%ld] partition_id %d, %ld < %ld ?\n", PartitionStatistics::my_machine_id(), partition_id, TG_DistributedVectorBase::ggb_pull_idx_per_machine_idx[partition_id], PER_THREAD_BUFF_SIZE / sizeof(node_t));
                //}
                //INVARIANT(TG_DistributedVectorBase::ggb_pull_idx_per_machine_idx[partition_id] < PER_THREAD_BUFF_SIZE / sizeof(node_t));
                TG_DistributedVectorBase::ggb_pull_idx_per_machine[partition_id][tmp_idx] = loc;
                //TG_DistributedVectorBase::ggb_pull_idx_per_machine[partition_id][TG_DistributedVectorBase::ggb_pull_idx_per_machine_idx[partition_id]] = loc;
                //TG_DistributedVectorBase::ggb_pull_idx_per_machine_idx[partition_id]++;
            } else {
                TG_DistributedVectorBase::ggb_pull_idx_overflow[partition_id] = true;
                ggb_pull_message_store_flags_per_machine[partition_id].TwoLevelBitMap<int64_t>::Set_Atomic_IfNotSet(loc);
            }
            AtomicOperation<T, op>(&global_gather_buffer[loc], msg.message);
#endif
        /* Incremental step 4 */
        } else if (UserArguments::INC_STEP == 4) {
            if (IsReaggregationRequired(msg.dst_vid)) {
                //PerformAtomicOperation(&global_gather_buffer[loc], msg.message, loc);
                // TODO do we need to flush OV in this case?
                _AccumulateNewAndOldMessagesIntoGGB(loc, msg.message);
                MarkReapplyRequiredByIdx(loc);
            }
        } else {
            //AtomicOperation<T, op>(&global_gather_buffer[loc], msg.message);
            _AccumulateNewAndOldMessagesIntoGGB(loc, msg.message);
        }
        //std::cout << vector_name << " After AccumulateMessageIntoGGB(" << msg.dst_vid << "), GGB: " << global_gather_buffer[loc] << "\n"; 
    }
#else
    /**
     * GGB message accumulate definition for old message transfer
     */
    void _AccumulateMessageIntoGGB(WritebackMessage<T>& msg, int partition_id=-1) {
        LOG_ASSERT(false);
        ALWAYS_ASSERT (global_gather_buffer_vid_range.contains(msg.dst_vid));
        int64_t loc = msg.dst_vid - global_gather_buffer_vid_range.GetBegin();
        //std::cout << vector_name << " AccumulateMessageIntoGGB(" << msg.dst_vid << "), Old: " << msg.old_message << ", New: " << msg.message << ", GGB: " << global_gather_buffer[loc] << "\n"; 
#ifdef INCREMENTAL_LOGGING
        if (INCREMENTAL_DEBUGGING_TARGET(msg.dst_vid)) {
            fprintf(stdout, "(%ld, %ld)\t[_AccumulateMessageIntoGGB]\tGGB[%ld] = %s <-- Msg[%ld] = %s, old_msg = %s, loc = %ld @ INC_STEP = %ld\n", (int64_t) UserArguments::UPDATE_VERSION, (int64_t) UserArguments::SUPERSTEP, (int64_t) msg.dst_vid, to_string_with_precision(global_gather_buffer[loc]).c_str(), (int64_t) msg.dst_vid, to_string_with_precision(msg.message).c_str(), to_string_with_precision(msg.old_message).c_str(), (int64_t) loc, (int64_t) UserArguments::INC_STEP);
        }
#endif
        /* Incremental step 1 */
        if (UserArguments::INC_STEP == -1) {
            MarkOvMessageReceived(loc); //XXX need?
            //T old_val = AtomicOperation<T, op>(&global_gather_buffer[loc], msg.message);
            AtomicOperation<T, op>(&global_gather_buffer[loc], msg.message);
            versioned_ov_array->MarkChanged(loc); //annotated originally
#ifndef GB_VERSIONED_ARRAY
            versioned_ov_array->MarkToFlush(loc);
#endif
        /* Incremental step 2 */
        } else if (UserArguments::INC_STEP == 2) {
            MarkOvMessageReceived(loc);
            if (!checkEquality(msg.message, msg.old_message)) {
#ifndef GB_VERSIONED_ARRAY
                versioned_ov_array->MarkToFlush(loc);
#else
                versioned_ov_array->MarkChanged(loc);
#endif
                MarkReapplyRequiredByIdx(loc);
            }
            _AccumulateNewAndOldMessagesIntoGGB(loc, msg.message, msg.old_message);
#ifdef PER_THREAD_GGB_PULL 
        /* Incremental step 3 for pull mode */
        } else if (UserArguments::INC_STEP == 3 && partition_id != -1) {
            ALWAYS_ASSERT(partition_id >= 0 && partition_id < PartitionStatistics::num_machines());
            ggb_pull_message_store_flags_per_machine[partition_id].TwoLevelBitMap<int64_t>::Set_Atomic_IfNotSet(loc);
            AtomicOperation<T, op>(&global_gather_buffer[loc], msg.message);
#endif
        /* Incremental step 4 */
        } else if (UserArguments::INC_STEP == 4) {
            if (IsReaggregationRequired(msg.dst_vid)) {
                PerformAtomicOperation(&global_gather_buffer[loc], msg.message, loc);
            }
        } else {
            AtomicOperation<T, op>(&global_gather_buffer[loc], msg.message);
        }
        //std::cout << vector_name << " After AccumulateMessageIntoGGB(" << msg.dst_vid << "), GGB: " << global_gather_buffer[loc] << "\n"; 
    }
#endif

	virtual void RespondOutputVectorMessagePushForIncProcessing(int64_t fromChunkID, int64_t chunkID, int partition_id, int tid, int idx, int64_t send_num,int64_t combined, int lv);
	
    /**
     * Accumulate message to GGB locally
     */
    inline void _AccumulateMessageIntoGGBInLocal(WritebackMessageWOOldMessage<T>& msg) {
        /*while (TG_NWSM_Base::read_ggb_flag.load() == 1L) {
            _mm_pause();
        }*/ // XXX

        int64_t temp_idx = TG_DistributedVectorBase::write_vec_changed_idx.fetch_add(1);
        if(temp_idx + 1 >= VECTOR_UPDATE_AGGREGATION_THRESHOLD) {
            TG_DistributedVectorBase::update_delta_buffer_overflow.store(true);
        }
        
        if(TG_DistributedVectorBase::update_delta_buffer_overflow.load()) {
            _AccumulateMessageIntoGGB(msg);
        } else {
            _AccumulateMessageIntoGGB(msg);
            TG_DistributedVectorBase::write_vec_changed[temp_idx] = msg.dst_vid;
        }
    }

        /**
         * Clear vector states
         */
		virtual void ResetStates() {
			rand_write_combined = 1;
			cur_chunk_merged = -1;
			small_message_idx = 0;
			spilled_msg_bytes.store(0L);
			prev_thread_buff_overflow = 0;
			thread_buff_overflow = 0;

			TG_DistributedVectorBase::write_vec_changed_idx.store(0L);
			TG_DistributedVectorBase::update_delta_buffer_overflow.store(false);

			//for (int64_t i = 0; i < PartitionStatistics::num_machines(); i++) {
			//    TG_DistributedVectorBase::ggb_pull_idx_per_machine_idx[i].idx = 0;
			//    TG_DistributedVectorBase::ggb_pull_idx_overflow[i] = false;
			//}
		}

	/*      APIs
	 *  - Update
	 *  - Read
	 *  - AccumulatedUpdate
	 */
    
    // called-by 'PULL_Scatter'
    // Read from LGB
    inline T PULL_ReadStoredMessage(node_t src_vid) {
        ALWAYS_ASSERT (UserArguments::INC_STEP == 4);
        ALWAYS_ASSERT (CheckTargetForPull());
        ALWAYS_ASSERT (UserArguments::CURRENT_LEVEL >= 0 && UserArguments::CURRENT_LEVEL < UserArguments::MAX_LEVEL);
        ALWAYS_ASSERT (win[UserArguments::CURRENT_LEVEL] != NULL);
        if (UserArguments::USE_DEGREE_ORDER_REPRESENTATION) {
            src_vid = PartitionStatistics::DegreeOrderToVid(src_vid);
        }
        return win[UserArguments::CURRENT_LEVEL]->PULL_ReadStoredMessage(src_vid);
    }
    
    /**
     * Clear generated message for pull mode
     */
    virtual void PULL_ClearGeneratedMessages() {
        ALWAYS_ASSERT (win[UserArguments::CURRENT_LEVEL] != NULL);

        T* vector_for_pull = win[UserArguments::CURRENT_LEVEL]->pull_send_buff;
        ALWAYS_ASSERT(vector_for_pull != NULL);

		turbo_timer tmr;
		tmr.start_timer(0);
        /*if (pull_flag) {
            int64_t pull_buffer_size = ((pull_buffer_idx * sizeof(WritebackMessage<T>)) / sizeof(T)) + 1;
#pragma omp parallel for
            for(int64_t i = 0; i < pull_buffer_size; i++) {
                OM_global_gather_buffer[i] = identity_element;
            }
        } else {*/
#ifdef PER_THREAD_GGB_PULL
#else
        ggb_pull_message_store_flags.TwoLevelBitMap<int64_t>::InvokeIfMarked([&](node_t idx) {
#ifdef INCREMENTAL_LOGGING
            node_t vid = idx + PartitionStatistics::my_first_node_id();
            if (INCREMENTAL_DEBUGGING_TARGET(vid)) {
                fprintf(stdout, "(%ld, %ld) [PULL_ClearGeneratedMessages] OmGgb[%ld] = %ld <- %ld\n", UserArguments::UPDATE_VERSION, UserArguments::SUPERSTEP, vid, vector_for_pull[idx], identity_element);
            }
#endif
            vector_for_pull[idx] = identity_element;
        });
        tmr.start_timer(2);
        ggb_pull_message_store_flags.ClearAll();
        tmr.stop_timer(2);
#endif
        //}
		tmr.stop_timer(0);
#ifndef PERFORMANCE
/*#pragma omp parallel for
				// XXX Useless code
        for(int64_t i = 0 ; i < PartitionStatistics::my_num_internal_nodes(); i++) {
            if (vector_for_pull[i] != identity_element) {
                fprintf(stdout, "[%ld] %s vector_for_pull[%ld] = %ld != identity elem\n", PartitionStatistics::my_machine_id(), this->vector_name.c_str(), i, (int64_t) vector_for_pull[i]);
            }
            ALWAYS_ASSERT(vector_for_pull[i] == identity_element);
        }*/
#endif
		tmr.start_timer(1);
		tmr.stop_timer(1);

#ifdef REPORT_PROFILING_TIMERS
        fprintf(stdout, "DONE [%ld] PULL_ClearGeneratedMessages %s, lgb_toggle = %ld / %.2f %.2f %.2f\n", PartitionStatistics::my_machine_id(), vector_name.c_str(), TG_DistributedVectorBase::lgb_toggle, tmr.get_timer(0), tmr.get_timer(1), tmr.get_timer(2));
#endif
    }

    // called-by 'GenerateMessage'
    // Write into OM_GGB
    /**
     * Check whether the operation is target for pull
     */
    virtual bool CheckTargetForPull() {
        return ((SeqRDWR != NOUSE) && (op == MIN || op == MAX) && (max_level == 1));
    }
    virtual void PULL_StoreMessageDefault(node_t vid) {
        PULL_StoreMessage(vid, Read(vid));
    }
    /**
     * Store message in pyull mode
     */
    inline void PULL_StoreMessage(node_t vid, T value) {
        //ALWAYS_ASSERT (RandRDWR == WRONLY || RandRDWR == RDWR);
        ALWAYS_ASSERT (UserArguments::CURRENT_LEVEL >= 0 && UserArguments::CURRENT_LEVEL < UserArguments::MAX_LEVEL);
        ALWAYS_ASSERT (win[UserArguments::CURRENT_LEVEL] != NULL);
        if (UserArguments::USE_DEGREE_ORDER_REPRESENTATION) {
            vid = PartitionStatistics::DegreeOrderToVid(vid);
        }
        ALWAYS_ASSERT (PartitionStatistics::my_first_node_id() <= vid && PartitionStatistics::my_last_node_id() >= vid);
        
        T* vector_for_pull = win[UserArguments::CURRENT_LEVEL]->pull_send_buff;
        ALWAYS_ASSERT(vector_for_pull != NULL);
        int64_t loc = vid - PartitionStatistics::my_first_node_id();
        
#ifdef INCREMENTAL_LOGGING
        if (INCREMENTAL_DEBUGGING_TARGET(vid)) {
            fprintf(stdout, "(%ld, %ld) [PULL_StoreMessage] OmGgb[%ld] = %ld <- %ld\n", UserArguments::UPDATE_VERSION, UserArguments::SUPERSTEP, vid, vector_for_pull[loc], value);
        }
#endif

        /*if (pull_flag) {
            ALWAYS_ASSERT((pull_buffer_idx + 1) * sizeof(WritebackMessage<T>) <= PartitionStatistics::max_num_nodes_per_vector_chunk() * sizeof(T));
            WritebackMessage<T>* temp_pull_wb_buffer = (WritebackMessage<T>*) OM_global_gather_buffer;
            temp_pull_wb_buffer[pull_buffer_idx].message = value;
            temp_pull_wb_buffer[pull_buffer_idx].dst_vid = vid;
            temp_pull_wb_buffer[pull_buffer_idx].old_message = identity_element;
            pull_buffer_idx++;
        } else {*/
        //ALWAYS_ASSERT(vector_for_pull[loc] == identity_element); //XXX ?
        vector_for_pull[loc] = value;
#ifndef PER_THREAD_GGB_PULL
        ggb_pull_message_store_flags.TwoLevelBitMap<int64_t>::Set_Atomic_IfNotSet(loc);
#endif
        //}
    }

#ifdef OldMessageTransfer
    /**
     * Update vid value at level LV
     */
    inline void Update(node_t vid, T value, T old_value = idenelem<T>(op)) {
        ALWAYS_ASSERT (RandRDWR == WRONLY || RandRDWR == RDWR);
        ALWAYS_ASSERT (UserArguments::CURRENT_LEVEL >= 0 && UserArguments::CURRENT_LEVEL < UserArguments::MAX_LEVEL);
        ALWAYS_ASSERT (win[UserArguments::CURRENT_LEVEL] != NULL);
        //if (UserArguments::USE_DEGREE_ORDER_REPRESENTATION) {
        //    vid = PartitionStatistics::DegreeOrderToVid(vid);
        //}
        win[UserArguments::CURRENT_LEVEL]->OvWrite(vid, value, old_value);
        return;
    }
	
    /**
     * Update vid value at level LV
     */
    template <int LV>
	inline void Update(node_t vid, T value, T old_value = idenelem<T>(op)) {
        ALWAYS_ASSERT (RandRDWR == WRONLY || RandRDWR == RDWR);
		ALWAYS_ASSERT (win[LV-1] != NULL);
		//if (UserArguments::USE_DEGREE_ORDER_REPRESENTATION) {
		//	vid = PartitionStatistics::DegreeOrderToVid(vid);
		//}
		win[LV-1]->OvWrite(vid, value, old_value);
	}    
#else
    /**
     * Update vid value at level LV
     */
	inline void Update(node_t vid, T value) {
        ALWAYS_ASSERT (RandRDWR == WRONLY || RandRDWR == RDWR);
		ALWAYS_ASSERT (UserArguments::CURRENT_LEVEL >= 0 && UserArguments::CURRENT_LEVEL < UserArguments::MAX_LEVEL);
		ALWAYS_ASSERT (win[UserArguments::CURRENT_LEVEL] != NULL);
		if (UserArguments::USE_DEGREE_ORDER_REPRESENTATION) {
			vid = PartitionStatistics::DegreeOrderToVid(vid);
		}
		win[UserArguments::CURRENT_LEVEL]->OvWrite(vid, value);
		return;
	}
	
    /**
     * Update vid value at level LV
     */
    template <int LV>
	inline void Update(node_t vid, T value) {
        ALWAYS_ASSERT (RandRDWR == WRONLY || RandRDWR == RDWR);
		ALWAYS_ASSERT (win[LV-1] != NULL);
		if (UserArguments::USE_DEGREE_ORDER_REPRESENTATION) {
			vid = PartitionStatistics::DegreeOrderToVid(vid);
		}
		win[LV-1]->OvWrite(vid, value);
	}
#endif

    /**
     * Read vid value at level LV
     */
	template <int LV>
	inline T& Read(node_t vid) {
		ALWAYS_ASSERT (win[LV-1] != NULL);
        if (UserArguments::USE_DEGREE_ORDER_REPRESENTATION) {
			vid = PartitionStatistics::DegreeOrderToVid(vid);
		}
		return win[LV-1]->IvRead(vid);
	}
    
    /**
     * Initialize vid as val
     */
    inline void Initialize(node_t vid, T val) {
        ALWAYS_ASSERT (win[UserArguments::CURRENT_LEVEL] != NULL);
        if (UserArguments::USE_DEGREE_ORDER_REPRESENTATION) {
            vid = PartitionStatistics::DegreeOrderToVid(vid);
        }
        // Mark dirty only if needed
        return win[UserArguments::CURRENT_LEVEL]->IvInitialize(vid, val);
    }
    
    /**
     * Initialize vid as val in previous version
     */
    inline void InitializePrev(node_t vid, T val) {
        ALWAYS_ASSERT (win[UserArguments::CURRENT_LEVEL] != NULL);
        if (UserArguments::USE_DEGREE_ORDER_REPRESENTATION) {
            vid = PartitionStatistics::DegreeOrderToVid(vid);
        }
        // Mark dirty only if needed
        return win[UserArguments::CURRENT_LEVEL]->IvInitializePrev(vid, val);
    }

    /**
     * Write vid value val (unsafe)
     */
    inline void Write(node_t vid, T val) {
        ALWAYS_ASSERT (win[UserArguments::CURRENT_LEVEL] != NULL);
        if (UserArguments::USE_DEGREE_ORDER_REPRESENTATION) {
            vid = PartitionStatistics::DegreeOrderToVid(vid);
        }
        // Mark dirty only if needed
        return win[UserArguments::CURRENT_LEVEL]->IvWrite(vid, val);
    }
    
    /**
     * Write vid value val (unsafe)
     */
    inline void WriteUnsafe(node_t vid, T val) {
        ALWAYS_ASSERT (win[UserArguments::CURRENT_LEVEL] != NULL);
        // Mark dirty only if needed
        return win[UserArguments::CURRENT_LEVEL]->IvWriteUnsafe(vid, val);
    }
    
    /**
     * Mark vid as dirty at input vector
     */
    inline void MarkDirty(node_t vid) {
        ALWAYS_ASSERT (win[UserArguments::CURRENT_LEVEL] != NULL);
        if (UserArguments::USE_DEGREE_ORDER_REPRESENTATION) {
            vid = PartitionStatistics::DegreeOrderToVid(vid);
        }
        return win[UserArguments::CURRENT_LEVEL]->IvMarkDirty(vid);
    }
    
    /**
     * Mark vid as dirty at input vector (unsafe)
     */
    inline void MarkDirtyUnsafe(node_t vid) {
        ALWAYS_ASSERT (win[UserArguments::CURRENT_LEVEL] != NULL);
        return win[UserArguments::CURRENT_LEVEL]->IvMarkDirtyUnsafe(vid);
    }
    
    /**
     * Read vid value of version t-1, superstep u from input vector
     */
    inline T& ReadPrev(node_t vid) {
		ALWAYS_ASSERT (UserArguments::CURRENT_LEVEL >= 0);
        ALWAYS_ASSERT (UserArguments::UPDATE_VERSION >= 1);
        ALWAYS_ASSERT (win[UserArguments::CURRENT_LEVEL] != NULL);
        if (UserArguments::USE_DEGREE_ORDER_REPRESENTATION) {
            vid = PartitionStatistics::DegreeOrderToVid(vid);
        }
        return win[UserArguments::CURRENT_LEVEL]->IvReadPrev(vid);
    }
    
    /**
     * Read vid value of version t superstep u-1 from input vector
     */
    inline T& ReadPrevSS(node_t vid) {
		ALWAYS_ASSERT (UserArguments::CURRENT_LEVEL >= 0);
        ALWAYS_ASSERT (win[UserArguments::CURRENT_LEVEL] != NULL);
        if (UserArguments::USE_DEGREE_ORDER_REPRESENTATION) {
            vid = PartitionStatistics::DegreeOrderToVid(vid);
        }
        return win[UserArguments::CURRENT_LEVEL]->IvReadPrevSS(vid);
    }
    
    /**
     * Read vid value of version t-1, superstep u-1 from input vector
     */
    inline T& ReadPrevSSSN(node_t vid) {
		ALWAYS_ASSERT (UserArguments::CURRENT_LEVEL >= 0);
        ALWAYS_ASSERT (UserArguments::UPDATE_VERSION >= 1);
        ALWAYS_ASSERT (win[UserArguments::CURRENT_LEVEL] != NULL);
        if (UserArguments::USE_DEGREE_ORDER_REPRESENTATION) {
            vid = PartitionStatistics::DegreeOrderToVid(vid);
        }
        return win[UserArguments::CURRENT_LEVEL]->IvReadPrevSSSN(vid);
    }

    /**
     * Read vid value of version t, superstep u from input vector
     */
    inline T& Read(node_t vid) {
		//if (win[UserArguments::CURRENT_LEVEL] == NULL) {
        //    fprintf(stdout, "[%ld] %s, %p %p %ld\n", PartitionStatistics::my_machine_id(), vector_name.c_str(), this, win[UserArguments::CURRENT_LEVEL], UserArguments::CURRENT_LEVEL);
        //}
        ALWAYS_ASSERT (UserArguments::CURRENT_LEVEL >= 0);
		ALWAYS_ASSERT (win[UserArguments::CURRENT_LEVEL] != NULL);
		if (UserArguments::USE_DEGREE_ORDER_REPRESENTATION) {
			vid = PartitionStatistics::DegreeOrderToVid(vid);
		}
		return win[UserArguments::CURRENT_LEVEL]->IvRead(vid);
	}
	
    /**
     * Return accumulated update for vid. The function calls
     * internal function AccumulatedUpdateInternal based on
     * template type T and op.
     */
    inline T AccumulatedUpdate(node_t vid) {
        return AccumulatedUpdateInternal<T, op>(vid);
    }

    template <typename T1, Op op1>  //kjhong
    inline T1 AccumulatedUpdateInternal(node_t vid) { 
        fprintf(stdout, "[%ld] op = %d, vec %s\n", PartitionStatistics::my_machine_id(), op, vector_name.c_str());
        LOG_ASSERT(false); 
    }

    /**
     * Returns whether vid value is equal with prev
     */
    virtual bool checkEqualityWithPrev(node_t vid) {
        return !checkEquality(ReadPrev(vid), Read(vid));
    }

    /**
     * Reset write buffer for vid
     */
	virtual void ResetWriteBuffer(node_t vid) {
		if (UserArguments::USE_DEGREE_ORDER_REPRESENTATION) {
			vid = PartitionStatistics::DegreeOrderToVid(vid);
		}
        /* Reset vid value in rand_write_buff_merged_vid_indexable */
		rand_write_buff_merged_vid_indexable[vid] = identity_element;
		return;
	}

    /**
     * Flush input vector and read and merge
     */
    virtual void FlushInputVectorAndReadAndMerge(int64_t vector_chunk_idx, int64_t superstep, const std::function<bool(node_t)>& iv_delta_write_if) {
        if (SeqRDWR == WRONLY || SeqRDWR == NOUSE) return;
        if (skip_iv) return;
        if (no_logging) return;
        ALWAYS_ASSERT(win[0] != NULL);
        syko_timer.start_timer(2);
        win[0]->FlushInputVectorAndReadAndMerge(vector_chunk_idx, superstep, iv_delta_write_if);
        syko_timer.stop_timer(2);
    }

    /**
     * Flush input vector for particular vector chunk and superstep
     */
	virtual void FlushInputVector(int64_t vector_chunk_idx, int64_t superstep, const std::function<bool(node_t)>& iv_delta_write_if) {
        if (SeqRDWR == WRONLY || SeqRDWR == NOUSE) return;
        if (skip_iv) return;
        if (no_logging) return;
        ALWAYS_ASSERT(win[0] != NULL);
        syko_timer.start_timer(2);
        win[0]->FlushInputVector(vector_chunk_idx, superstep, iv_delta_write_if);
        syko_timer.stop_timer(2);
    }

    /**
     * Flush input vector for (update, superstep)
     */
    virtual void FlushInputVector(int update_version, int superstep_version) {
        if (skip_iv) return;
        if (no_logging) return;
        win[0]->FlushInputVector(update_version, superstep_version);
    }

    // TODO not used
	virtual void FlushSequentialVector(int64_t start_part_id, int64_t part_id) {
	    // Doing Nothing
    }
	
    /**
     * Read input vector at apply phase for first level window
     */
    virtual void ReadInputVectorsAtApplyPhase (int vector_chunk_idx) {
        if (SeqRDWR == WRONLY || SeqRDWR == NOUSE) return;
        if (no_logging) return;
        if (skip_iv) return;
        syko_timer.start_timer(0);
        /* Call function for first level window */
        win[0]->ReadInputVectorsAtApplyPhase(vector_chunk_idx);
        syko_timer.stop_timer(0);
    }

    /**
     * Read input vector for given partition 
     */
	virtual void ReadInputVector(int64_t start_part_id, int64_t part_id, int64_t next_part_id, int ss) {
		ALWAYS_ASSERT (part_id >= 0 && part_id < PartitionStatistics::num_target_vector_chunks());
		ALWAYS_ASSERT (UserArguments::CURRENT_LEVEL >= 0 && UserArguments::CURRENT_LEVEL < UserArguments::MAX_LEVEL);
		if (win[UserArguments::CURRENT_LEVEL] == NULL) return;
        if (skip_iv) return;
        if (no_logging) return;

        /* Read input vector for current level */
        syko_timer.start_timer(1);
		win[UserArguments::CURRENT_LEVEL]->ReadInputVector(start_part_id, part_id, next_part_id, ss);
        syko_timer.stop_timer(1);

        /* Update input vector partition info and range */
		cur_src_vector_part = part_id;
		cur_src_vector_part_machine = part_id / UserArguments::VECTOR_PARTITIONS;
		cur_src_vector_part_chunk = part_id % UserArguments::VECTOR_PARTITIONS;
		cur_src_vector_vid_range = PartitionStatistics::machine_id_and_chunk_idx_to_vid_range(cur_src_vector_part_machine, cur_src_vector_part_chunk);
		seq_read_vector_vid_indexable = seq_read_vector - PartitionStatistics::per_machine_vid_range(PartitionStatistics::my_machine_id()).GetBegin();
		seq_write_vector_vid_indexable = seq_write_vector - PartitionStatistics::per_machine_vid_range(PartitionStatistics::my_machine_id()).GetBegin();
	}

    // TODO not used since its caller is not used
	virtual void ReadSpilledWritesAndMerge(int64_t to_chunk_id, char* buf) {
		ALWAYS_ASSERT (global_gather_buffer != NULL);
		ALWAYS_ASSERT (buf != NULL);
		
        Range<node_t> vid_range = PartitionStatistics::per_edge_partition_vid_range(PartitionStatistics::my_machine_id() * UserArguments::VECTOR_PARTITIONS + to_chunk_id);
		node_t first_vid = vid_range.GetBegin();
		T iden = idenelem<T>(operation);
		T* merge_buffer = (T*) buf;

		if (global_gather_buffer_vid_range.Overlapped(vid_range)) {
			Range<node_t> vid_range_merged_in_memory = global_gather_buffer_vid_range.Intersection(vid_range);

			#pragma omp parallel num_threads(NumaHelper::num_cores() - UserArguments::NUM_THREADS)
			{
                if (UserArguments::UPDATE_VERSION == 0) {
                    // [begin, begin_overlap)
				    #pragma omp for nowait
                    for (node_t vid = global_gather_buffer_vid_range.begin; vid < vid_range_merged_in_memory.begin; vid++) {
                        node_t idx_from = vid - global_gather_buffer_vid_range.begin;
                        global_gather_buffer[idx_from] = iden;
                    }

                    // [begin_overlap, end_overlap]
				    #pragma omp for nowait
                    for (node_t vid = vid_range_merged_in_memory.begin; vid <= vid_range_merged_in_memory.end; vid++) {
                        node_t idx_from = vid - global_gather_buffer_vid_range.begin;
                        node_t idx_to = vid - first_vid;
                        //      merge_buffer[idx_to] = global_gather_buffer[idx_from];
                        Operation<T, op>(&merge_buffer[idx_to], global_gather_buffer[idx_from]);
                        global_gather_buffer[idx_from] = iden;
                    }

                    // (end_overlap, end]
				    #pragma omp for nowait
                    for (node_t vid = vid_range_merged_in_memory.end + 1; vid <= global_gather_buffer_vid_range.end; vid++) {
                        node_t idx_from = vid - global_gather_buffer_vid_range.begin;
                        global_gather_buffer[idx_from] = iden;
                    }
                } else {
                    // [begin, begin_overlap)
				    #pragma omp for nowait
                    for (node_t vid = global_gather_buffer_vid_range.begin; vid < vid_range_merged_in_memory.begin; vid++) {
                        node_t idx_from = vid - global_gather_buffer_vid_range.begin;
                        global_gather_buffer[idx_from] = iden;
                    }

                    // [begin_overlap, end_overlap]
				    #pragma omp for nowait
                    for (node_t vid = vid_range_merged_in_memory.begin; vid <= vid_range_merged_in_memory.end; vid++) {
                        node_t idx_from = vid - global_gather_buffer_vid_range.begin;
                        node_t idx_to = vid - first_vid;
                        //      merge_buffer[idx_to] = global_gather_buffer[idx_from];
                        Operation<T, op>(&merge_buffer[idx_to], global_gather_buffer[idx_from]);
                        global_gather_buffer[idx_from] = iden;
                    }

                    // (end_overlap, end]
				    #pragma omp for nowait
                    for (node_t vid = vid_range_merged_in_memory.end + 1; vid <= global_gather_buffer_vid_range.end; vid++) {
                        node_t idx_from = vid - global_gather_buffer_vid_range.begin;
                        global_gather_buffer[idx_from] = iden;
                    }
                }
			}
		}

#ifndef PERFORMANCE
		#pragma omp parallel for
		for (int64_t i = 0; i < global_gather_buffer_vid_range.length(); i++) {
			ALWAYS_ASSERT (global_gather_buffer[i] == idenelem<T>(operation));
		}
#endif

		// We can skip it if there is no spilled message.
		if (spilled_msg_bytes.load() > 0) {
			#pragma omp parallel for num_threads(NumaHelper::num_cores() - UserArguments::NUM_THREADS) schedule(dynamic, 1)
			for (int j = 0; j < PartitionStatistics::num_target_vector_chunks(); j++) {
				ALWAYS_ASSERT (to_chunk_id >= 0 && to_chunk_id < UserArguments::VECTOR_PARTITIONS);
				ALWAYS_ASSERT (j >= 0 && j < PartitionStatistics::num_target_vector_chunks());
				int64_t file_size = updates_spilling_handler[to_chunk_id][j].file_size();
				if (file_size == 0) continue;
				WritebackMessage<T>* msg_buffer = (WritebackMessage<T>*) updates_spilling_handler[to_chunk_id][j].CreateMmap(false);
				ALWAYS_ASSERT (msg_buffer != NULL);

				int64_t num_entries = file_size / sizeof(WritebackMessage<T>);
				for (int64_t k = 0; k < num_entries; k++) {
					ALWAYS_ASSERT (PartitionStatistics::per_edge_partition_vid_range(PartitionStatistics::my_machine_id()
					               * UserArguments::VECTOR_PARTITIONS + to_chunk_id).contains(msg_buffer[k].dst_vid));

					node_t lid = msg_buffer[k].dst_vid - first_vid;
					AtomicOperation<T, op>(&merge_buffer[lid], msg_buffer[k].message);
				}
				updates_spilling_handler[to_chunk_id][j].DestructMmap();
				updates_spilling_handler[to_chunk_id][j].Truncate(0);
			}
		}
	}

    /**
     * Read and merge spilled writes from disk
     */
	virtual void ReadAndMergeSpilledWritesFromDisk(int64_t to_chunk_id) {
		if(!(RandRDWR == WRONLY || RandRDWR == RDWR)) return;
		ALWAYS_ASSERT (win[0] != NULL);
        /* Updated merged chunk count */
		cur_chunk_merged++;

		// XXX - Optimization for Gather+Apply
        /* In this function, we assume that all GGB data is fully aggregated in memory */
        bool fully_aggregated_in_memory = (global_gather_buffer_vid_range.length() == PartitionStatistics::my_num_internal_nodes());
        INVARIANT (fully_aggregated_in_memory && UserArguments::VECTOR_PARTITIONS == 1);
        
        /* Update merged output vector write buffer and vid ranges */
        rand_write_buff_merged = global_gather_buffer;
        rand_write_buff_merged_vid_range = PartitionStatistics::per_edge_partition_vid_range(PartitionStatistics::my_machine_id() * UserArguments::VECTOR_PARTITIONS + cur_chunk_merged);
        rand_write_buff_merged_vid_indexable = rand_write_buff_merged - rand_write_buff_merged_vid_range.GetBegin();
    }

    /**
     * Spill vector write into disk
     */
	virtual void SpillVectorWriteIntoDisk(char* data, int64_t size_to_append, int64_t from_machine_id, int64_t from_chunk_id, int64_t to_chunk_id) {
		ALWAYS_ASSERT (to_chunk_id >= 0 && to_chunk_id < UserArguments::VECTOR_PARTITIONS);
		ALWAYS_ASSERT (from_machine_id * UserArguments::VECTOR_PARTITIONS + from_chunk_id >= 0 && from_machine_id * UserArguments::VECTOR_PARTITIONS + from_chunk_id < PartitionStatistics::num_target_vector_chunks());
		/* Append data to spill handler */
        updates_spilling_handler[to_chunk_id][from_machine_id * UserArguments::VECTOR_PARTITIONS + from_chunk_id].Append(size_to_append, data);
		/* Accumulate spilled disk vector write bytes */
        TG_DistributedVectorBase::AccumulateSpilledDiskVectorWrites(size_to_append);
        /* Update total spilled bytes */
		spilled_msg_bytes.fetch_add(size_to_append);
	}

    /**
     * Pin output vector write buffer in memory
     */
	virtual void PinRandVectorWriteBufferInMemory() {
		if(!(RandRDWR == WRONLY || RandRDWR == RDWR)) return;
		ALWAYS_ASSERT (global_gather_buffer == NULL);
		int64_t num_entries = 0;
            
        std::string ov_ss_path = ov_path + "_superstep" + std::to_string(0);
        versioned_ov_array->Create();

		num_entries = PartitionStatistics::max_num_nodes_per_vector_chunk();
		global_gather_buffer = (T*) versioned_ov_array->GetBuffer();
		node_t begin_vid = PartitionStatistics::my_first_node_id();
		node_t end_vid = begin_vid + num_entries - 1;
		global_gather_buffer_vid_range.Set(begin_vid, end_vid);
		ALWAYS_ASSERT (num_entries == global_gather_buffer_vid_range.length()); // ASSUMED ...
		
        OM_global_gather_buffer = (T*) NumaHelper::alloc_numa_interleaved_memory(sizeof(T) * num_entries);
        //OM_global_gather_buffer = (T*) NumaHelper::alloc_numa_local_memory(sizeof(T) * num_entries);
        ggb_msg_received_flags.Init(num_entries);

        bool fully_aggregated_in_memory = (global_gather_buffer_vid_range.length() == PartitionStatistics::my_num_internal_nodes());
        INVARIANT (fully_aggregated_in_memory && UserArguments::VECTOR_PARTITIONS == 1);
        rand_write_buff_merged = global_gather_buffer;
        rand_write_buff_merged_vid_range = global_gather_buffer_vid_range;
        rand_write_buff_merged_vid_indexable = rand_write_buff_merged - rand_write_buff_merged_vid_range.GetBegin();

		#pragma omp parallel for
		for (node_t lid = 0; lid < global_gather_buffer_vid_range.length(); lid++) {
			global_gather_buffer[lid] = idenelem<T>(operation);
			OM_global_gather_buffer[lid] = idenelem<T>(operation);
			//versioned_ov_array->MarkDirty(lid);
			//versioned_ov_array->MarkChanged(lid); //XXX tslee added
			//versioned_ov_array->MarkToFlush(lid);
        }
        LocalStatistics::register_mem_alloc_info((vector_name + " PinRandVectorWriteBufferInMemory (OM_GGB + ggb_msg_received_flags)").c_str(), (sizeof(T) * num_entries + ggb_msg_received_flags.total_container_size()) / (1024*1024L));
        
        TG_DistributedVectorBase::PinRandVectorWriteBufferInMemory();
	}

	
	virtual bool InitializeOutputVector(int64_t start_part_id, int64_t part_id, int64_t next_part_id);
    
    /**
     * Pull input vector
     */
    virtual void PullInputVector(int64_t start_part_id, int64_t part_id, int64_t next_part_id) {
        if (win[UserArguments::CURRENT_LEVEL] == NULL) {
            return;
        }
        win[UserArguments::CURRENT_LEVEL]->PullInputVector(start_part_id, part_id, next_part_id);
    }
	
    /**
     * Clear pulled message in LGB
     */
    virtual void ClearPulledMessagesInLocalGatherBuffer(int64_t start_part_id, int64_t part_id, int64_t next_part_id, int64_t edges_cnt) {
        if (win[UserArguments::CURRENT_LEVEL] == NULL) {
			return;
		}
		win[UserArguments::CURRENT_LEVEL]->ClearPulledMessagesInLocalGatherBuffer(start_part_id, part_id, next_part_id, edges_cnt);
    }

    /**
     * Flush LGB. Returns whether the overflow has occured.
     */
	virtual bool FlushLocalGatherBuffer(int64_t start_part_id, int64_t part_id, int64_t next_dst_part_id, int64_t edges_cnt) {
		if (win[UserArguments::CURRENT_LEVEL] == NULL) {
			return false;
		}
		return win[UserArguments::CURRENT_LEVEL]->FlushLocalGatherBuffer(start_part_id, part_id, next_dst_part_id, edges_cnt);
	}

    /**
     * Flush update buffer
     */
	virtual void FlushUpdateBuffer(int64_t start_part_id, int64_t part_id, int64_t next_part_id, int64_t subchunk_idx, int64_t overflow_cnt) {
		ALWAYS_ASSERT (UserArguments::CURRENT_LEVEL >= 0 && UserArguments::CURRENT_LEVEL < UserArguments::MAX_LEVEL);
		if (win[UserArguments::CURRENT_LEVEL] == NULL) {
			return;
		}
		win[UserArguments::CURRENT_LEVEL]->FlushUpdateBuffer(start_part_id, part_id, next_part_id, subchunk_idx, overflow_cnt);
	}

    /**
     * Flush update buffer per thread. Returns whether the overflow has occured.
     */
	virtual bool FlushUpdateBufferPerThread(int64_t start_part_id, int64_t part_id, int64_t next_part_id, int64_t subchunk_idx) {
		ALWAYS_ASSERT (UserArguments::CURRENT_LEVEL >= 0 && UserArguments::CURRENT_LEVEL < UserArguments::MAX_LEVEL);
		if (win[UserArguments::CURRENT_LEVEL] == NULL) {
			return false;
		}
		return win[UserArguments::CURRENT_LEVEL]->FlushUpdateBufferPerThread(start_part_id, part_id, next_part_id, subchunk_idx);
	}
    
    /**
     * Returns whether the vector is updatable
     */
    virtual bool IsUpdatable() {
		if(RandRDWR == WRONLY || RandRDWR == RDWR) return true;
        return false;
    }
    
    // TODO not used
    virtual void PruneReaggregationRequired() {
        LOG_ASSERT(false); // XXX not being used, why?
        
        if (no_logging) return;
        ggb_msg_received_flags.InvokeIfMarked([&](node_t loc) {
            node_t vid = loc + global_gather_buffer_vid_range.GetBegin();
        });
    }
    
    /**
     * Set reapply reaggregation for vid
     */
    virtual void PruneReaggregationRequired(node_t vid) {
        if (no_logging) return;
        if (!IsOvMessageReceived(vid)) return;
        
        int64_t loc = vid - global_gather_buffer_vid_range.GetBegin();
        T new_ggb_val = global_gather_buffer[loc];
        _AccumulateNewAndOldMessages(&new_ggb_val, OM_global_gather_buffer[loc]);
        /** If values are different, mark reapply */
        if (!checkEquality(global_gather_buffer[loc], new_ggb_val)) {
            global_gather_buffer[loc] = new_ggb_val;
#ifndef GB_VERSIONED_ARRAY
            versioned_ov_array->MarkToFlush(loc);
#else
            versioned_ov_array->MarkChanged(loc);
#endif
            MarkReapplyRequiredByIdx(loc);
        }
        //bool required = CheckOpRequiresReaggregationForDeletion(global_gather_buffer[loc], OM_global_gather_buffer[loc]);
        bool required = CheckOpRequiresReaggregationForDeletion<T, op>(global_gather_buffer[loc]);
 
        //if (vid == 13019899) {
		//if (UserArguments::UPDATE_VERSION == 2 && UserArguments::SUPERSTEP == 4 && required) {
		//if (UserArguments::UPDATE_VERSION == 1 && required) {
	    //    fprintf(stdout, "[%ld] (%ld, %ld)\t[PruneReaggregationRequired]\t ggb[%ld] = %ld, cnt = %ld, om_ggb[%ld] = %ld, cnt = %ld, reagg_required = %ld @ INC_STEP = %ld\n", PartitionStatistics::my_machine_id(), (int64_t) UserArguments::UPDATE_VERSION, (int64_t) UserArguments::SUPERSTEP, (int64_t) vid, (int64_t) GET_VAL_64BIT(global_gather_buffer[loc]), (int64_t) GET_CNT_64BIT(global_gather_buffer[loc]), (int64_t) vid, (int64_t) GET_VAL_64BIT(OM_global_gather_buffer[loc]), (int64_t) GET_CNT_64BIT(OM_global_gather_buffer[loc]), (int64_t) required? 1L : 0L, (int64_t) UserArguments::INC_STEP);
        //}
#ifdef INCREMENTAL_LOGGING
        if (INCREMENTAL_DEBUGGING_TARGET(vid)) {
            fprintf(stdout, "(%ld, %ld)\t[PruneReaggregationRequired]\t ggb[%ld] = %s, om_ggb[%ld] = %s, reagg_required = %ld @ INC_STEP = %ld\n", (int64_t) UserArguments::UPDATE_VERSION, (int64_t) UserArguments::SUPERSTEP, (int64_t) vid, std::to_string(global_gather_buffer[loc]).c_str(), (int64_t) vid, std::to_string(OM_global_gather_buffer[loc]).c_str(), (int64_t) required? 1L : 0L, (int64_t) UserArguments::INC_STEP);
        }
#endif       
        
        /** Mark reaggregation for vid  */
        if (required) {
            MarkReaggregationRequired(vid);
        } else {
            //_MergeGgbAndOmGgb(loc);
            ClearReaggregationRequired(vid);
        }

        OM_global_gather_buffer[loc] = identity_element;
        ClearOvMessageReceived(vid);
    }

    /**
     * Returns whether reaggregation is required for vid
     */
    virtual bool IsReaggregationRequired(node_t vid) {
        ALWAYS_ASSERT (UserArguments::INCREMENTAL_PROCESSING);
        ALWAYS_ASSERT (!no_logging);
        if (!IsUpdatable() && !(UserArguments::USE_PULL && CheckTargetForPull())) return false;
        /* Fetch flag from ggb_reaggregation_flags */
        return (ggb_reaggregation_flags.Get(vid - PartitionStatistics::my_first_node_id()));
    }

    /**
     * Returns if reapply required for vid
     */
    virtual bool IsReapplyRequired(node_t vid) {
        ALWAYS_ASSERT (UserArguments::INCREMENTAL_PROCESSING);
        ALWAYS_ASSERT (!no_logging);
        if (!IsUpdatable()) return false;
        /* Fetch flag from ggb_reapply_flags */
        return (ggb_reapply_flags.Get(vid - PartitionStatistics::my_first_node_id()));
    }
    
    /**
     * Clear output vector flags
     */
    virtual void ClearOvFlagsForIncrementalProcessing() {
        if (!UserArguments::INCREMENTAL_PROCESSING) return;

        if((RandRDWR == WRONLY || RandRDWR == RDWR)) {
            //ggb_reapply_flags.ClearAll();
            ggb_reaggregation_flags.ClearAll();
            //versioned_ov_array->ClearAllDirty();
        }
    }
    
    /**
     * Clear input vector flags
     */
    virtual void ClearIvFlagsForIncrementalProcessing(bool clear_dirty, bool clear_changed) {
        if (SeqRDWR != NOUSE) {
            ALWAYS_ASSERT(win[0] != NULL);
            win[0]->ClearFlagsForIncrementalProcessing(clear_dirty, clear_changed);
        }
    }
    
    /**
     * Returns whether input vector has element for version u
     * and superstep s
     */
    virtual bool hasIvVersion(int u, int s) {
        if (win[0] == NULL) return false;
        return win[0]->hasIvVersion(u, s);
    }
    
    /**
     * Returns number of input vector supersteps for version u
     */
    virtual int64_t GetNumIvSuperstepVersions(int u) {
        if (win[0] == NULL) return 0;
		if (SeqRDWR == NOUSE) return 0;
        if (no_logging) return 0;
        return win[0]->GetNumIvSuperstepVersions(u);
    }

    /**
     * Set whether to skip reading input vector while processing
     */
    virtual void SetSkipIv(bool skip) { skip_iv = skip; }
    

// TODO not using and strange logic 

#ifdef OldMessageTransfer
    bool CheckOpRequiresReaggregation(T agg_val, T new_upd_val, T old_upd_val) {
        LOG_ASSERT(false);
        //return CheckOpRequiresReaggregationForInsertion(agg_val, new_upd_val) || CheckOpRequiresReaggregationForDeletion(agg_val, old_upd_val);
    }

#else

    /**
     * Returns reaggregation necessity for op
     */
    bool CheckOpRequiresReaggregation(T agg_val, T upd_val, bool ins_or_del) {
        if (ins_or_del) {
            return CheckOpRequiresReaggregationForInsertion(agg_val, upd_val);
        } else {
            return CheckOpRequiresReaggregationForDeletion(agg_val, upd_val);
        }
    }
#endif

    /**
     * Returns reaggregation necessity for op in insertion
     */
    bool CheckOpRequiresReaggregationForInsertion(T agg_val, T upd_val) {
        switch (op) {
            case MAX:
            case MIN:
            case PLUS:
            case MULTIPLY:
                return false;
            default:
                return true;
        }
    }

    /**
     * Returns reaggregation necessity for op in deletion
     */
    template <typename T1, Op op1> 
    bool CheckOpRequiresReaggregationForDeletion(T1& agg_val) { 
        if (op1 == MIN || op1 == MAX) LOG_ASSERT(false); // XXX remove this line
        return false;
        /*switch (op) {
            case MAX:
            case MIN:
                if (((int) GET_CNT_64BIT(agg_val)) <= 0) return true;
                else return false;
            case MULTIPLY:
            case PLUS:
                return false;
            default:
                return true;
        }*/
    }
 
    // TODO not using
    inline void PerformAtomicOperation(T* dst, T& val, node_t loc) {
#ifdef INCREMENTAL_LOGGING
        if (INCREMENTAL_DEBUGGING_TARGET(loc + PartitionStatistics::my_first_node_id())) {
            fprintf(stdout, "(%ld, %ld)\t[PerformAtomicOperation OV]\tMsg[%ld] = %ld <- %ld, loc = %ld @ INC_STEP = %ld\n", (int64_t) UserArguments::UPDATE_VERSION, (int64_t) UserArguments::SUPERSTEP, (int64_t) loc + PartitionStatistics::my_first_node_id(), (int64_t) *dst, (int64_t) val, (int64_t) loc, (int64_t) UserArguments::INC_STEP);
        }
#endif

        T old_val = AtomicOperation<T, op>(dst, val);
        //versioned_ov_array->MarkDirty(loc);
        //versioned_ov_array->MarkChanged(loc);
        //versioned_ov_array->MarkToFlush(loc);
        MarkReapplyRequiredByIdx(loc);
    }

    /**
     * Returns if input vector is dirty for vid
     */
    virtual bool IsIvDirty(node_t vid) { 
        if (SeqRDWR == NOUSE) return false;
        if (win[UserArguments::CURRENT_LEVEL] == NULL) return false;
        return win[0]->IsIvDirty(vid); 
    }
    /**
     * Mark vid as changed in input vector
     */
    virtual void MarkIvChanged(node_t vid) {
        if (SeqRDWR == NOUSE) return;
        if (win[UserArguments::CURRENT_LEVEL] == NULL) return;
        ALWAYS_ASSERT(win[0] != NULL);
        win[0]->MarkIvChanged(vid);
    }
    /**
     * Mark vid as dirty in input vector
     */
    virtual void MarkIvDirty(node_t vid) {
        if (SeqRDWR == NOUSE) return;
        if (win[UserArguments::CURRENT_LEVEL] == NULL) return;
        ALWAYS_ASSERT(win[0] != NULL);
        win[0]->MarkIvDirty(vid);
    }
    /**
     * Clear vid dirty flag in input vector
     */
    virtual void ClearIvDirty(node_t vid) {
        if (SeqRDWR == NOUSE) return;
        if (win[UserArguments::CURRENT_LEVEL] == NULL) return;
        ALWAYS_ASSERT(win[0] != NULL);
        win[0]->ClearIvDirty(vid);
    }
    
    /**
     * Get input vector dirty map
     */
    virtual TwoLevelBitMap<node_t>* GetIvDirtyBitMap() {
        ALWAYS_ASSERT(SeqRDWR != NOUSE);
        return win[0]->GetIvDirtyBitMap();
    }
    
    /**
     * Whether the vector has input vector
     */
    virtual bool HasIv() {
        return (SeqRDWR != NOUSE);
    }
    
    /**
     * Whether the vector has output vector
     */
    virtual bool HasOv() {
        return (RandRDWR == WRONLY || RandRDWR == RDWR);
    }
    
    /**
     * Whether using differential semantics
     */
    virtual bool IsOvCachingAggregator() {
        return use_diff_semantics;
    }
    
    /**
     * Get input vector changed map
     */
    virtual TwoLevelBitMap<node_t>* GetIvChangedBitMap() {
        ALWAYS_ASSERT(SeqRDWR != NOUSE);
        return win[0]->GetIvChangedBitMap();
    }
    
    /**
     * Get output vector next bitmap
     */
    virtual TwoLevelBitMap<node_t>* GetOvNextBitMap(bool t) {
		if(!(RandRDWR == WRONLY || RandRDWR == RDWR)) return NULL;
        return versioned_ov_array->GetNextFlags(t);
    }
    
    /**
     * Get first snapshot flags for output vector
     */
    virtual TwoLevelBitMap<node_t>* GetOvFirstSnapshotBitMap(bool t) {
		if(!(RandRDWR == WRONLY || RandRDWR == RDWR)) return NULL;
        return versioned_ov_array->GetFirstSnapshotFlags(t);
    }
    
    // TODO not used
    virtual void MarkOvDirty(node_t vid) { 
		if(!(RandRDWR == WRONLY || RandRDWR == RDWR)) return;
        node_t idx = vid - PartitionStatistics::my_first_internal_vid();
        versioned_ov_array->MarkDirty(idx); 
    }
    // TODO not used
    virtual void ClearOvDirty(node_t vid) { 
		if(!(RandRDWR == WRONLY || RandRDWR == RDWR)) return;
        node_t idx = vid - PartitionStatistics::my_first_internal_vid();
        versioned_ov_array->ClearDirty(idx); 
    }
    // TODO not used
    virtual void ClearOvChanged(node_t vid) { 
		if(!(RandRDWR == WRONLY || RandRDWR == RDWR)) return;
        node_t idx = vid - PartitionStatistics::my_first_internal_vid();
        versioned_ov_array->ClearChanged(idx); 
    }
    /**
     * Clear all dirty output vector
     */
    virtual void ClearAllOvDirty() { 
        if(!(RandRDWR == WRONLY || RandRDWR == RDWR)) return;
        versioned_ov_array->ClearAllDirty(); 
    }
    /**
     * Clear all changed output vector
     */
    virtual void ClearAllOvChanged() { 
        if(!(RandRDWR == WRONLY || RandRDWR == RDWR)) return;
        versioned_ov_array->ClearAllChanged(); 
    }
    
    /**
     * Set current version of input vector as version u and superstep s.
     */
    virtual void SetCurrentVersion(int u, int s) {
        if (SeqRDWR == NOUSE) return;
        if (win[UserArguments::CURRENT_LEVEL] == NULL) return;
        ALWAYS_ASSERT(win[0] != NULL);
        win[0]->SetCurrentVersion(u, s);
    }
    
    /**
     * Asynchronously call to construct version for version u, superstep s
     */
    virtual void CallConstructNextSuperStepArrayAsync(int u, int s, bool run_static_processing) {
        if (win[UserArguments::CURRENT_LEVEL] == NULL) return;
        if (no_logging) return;
        ALWAYS_ASSERT(win[0] != NULL);
        /* Call for output vector */
        versioned_ov_array->CallConstructNextSuperStepArrayAsync(u, s, run_static_processing);
        /* Call for input vector */s
        win[0]->CallConstructNextSuperStepArrayAsync(u, s, run_static_processing);
    }
    
    /**
     * Process pending async IO requests
     */
    virtual void ProcessPendingAioRequests() {
        /* Process for output vector */
        if (RandRDWR != NOUSE) {
            versioned_ov_array->WaitForIoRequests(true, true);
        }
        if (SeqRDWR != NOUSE) {
            ALWAYS_ASSERT(win[0] != NULL);
            /* Process for input vector */
            win[0]->ProcessPendingAioRequests();
        }
    }

    /* Print out timer values */
    virtual void PrintTimers() {
        //fprintf(stdout, "[%ld] Vector(%s)\t%.2f\t%.2f\t%.2f\t%.2f\t%.2f\t%.2f\t%.2f\t%.2f\t%.2f\t%.2f\n", PartitionStatistics::my_machine_id(), vector_name.c_str(), syko_timer.get_timer(0), syko_timer.get_timer(1), syko_timer.get_timer(2), syko_timer.get_timer(3), syko_timer.get_timer(4), syko_timer.get_timer(5), 0.0, syko_timer.get_timer(6), syko_timer.get_timer(7), syko_timer.get_timer(8));
        //ResetTimers();
    }
    
    /* Reset timers */
    virtual void ResetTimers() {
        for (int i = 0; i < 9; i++) { syko_timer.reset_timer(i); }
    }

    /**
     * Get read buffer size for input vector
     */
    virtual int64_t GetSeqVectorReadBufferSize() {
		if(RandRDWR == RDONLY || RandRDWR == RDWR) {
			return 1 + (sizeof(T) * PartitionStatistics::my_num_internal_nodes() / (1024*1024L));
		} else {
            /* No buffer when not in read mode */
			return 0;
		}
	}
    /**
     * Get write buffer size for output vector
     */
	virtual int64_t GetRandVectorWriteBufferSize() {
		if(RandRDWR == WRONLY || RandRDWR == RDWR) {
			return 1 + (2 * sizeof(T) * PartitionStatistics::max_num_nodes_per_vector_chunk()) / (1024*1024L);
		} else {
            /* No buffer when not in write mode */
			return 0;
		}
	}
    /**
     * Get receive buffer size for output vector
     */
	virtual int64_t GetRandVectorWriteReceiveBufferSize() {
		if(RandRDWR == WRONLY || RandRDWR == RDWR) {
			return 1 + (1 * sizeof(T) * PartitionStatistics::max_num_nodes_per_vector_chunk() / (1024*1024L));
		} else {
			return 0;
		}
	}
    /**
     * Get total size of vector window
     */
	virtual int64_t GetSizeOfVW() {
		int64_t bytes = 0;
		for (int lv = 0; lv < UserArguments::MAX_LEVEL; lv++) {
		//for (int lv = 0; lv < max_level; lv++) {
            //fprintf(stdout, "%s win[%d] getsizeofvw\n", vector_name.c_str(), lv);
			if (win[lv] == NULL) continue;
            /* Accumulate values */
			bytes += win[lv]->GetSizeOfVW();
		}
		return bytes;
	}
    /**
     * Get total size of global ggb and OM_ggb
     */
	virtual int64_t GetSizeOfGGB() {
		int64_t bytes = 0;
        if ((RandRDWR == WRONLY || RandRDWR == RDWR)) {
            /* Add size of version_ov_array and OM_ggb */
            bytes += versioned_ov_array->GetMemoryUsage(); // ggb
            bytes += (sizeof(T) * PartitionStatistics::max_num_nodes_per_vector_chunk()); //OM_ggb
        }
		return bytes;
	}
    /**
     * Get total size of LGB
     */
	virtual int64_t GetSizeOfLGB() {
		int64_t bytes = 0;
		for (int lv = 0; lv < UserArguments::MAX_LEVEL; lv++) {
		//for (int lv = 0; lv < max_level; lv++) {
			if (win[lv] == NULL) continue;
			bytes += win[lv]->GetSizeOfLGB();
		}
		return bytes;
	}
    
    /**
     * Get total delta size stored in disk in given version interval
     */
    virtual int64_t GetTotalDeltaSizeInDisk(Version from, Version to) {
        if (no_logging) return 0;
        int64_t bytes = 0;
        /* Get for versioned_iv_array */
        for (int lv = 0; lv < UserArguments::MAX_LEVEL; lv++) {
        //for (int lv = 0; lv < max_level; lv++) {
            if (win[lv] == NULL) continue;
            bytes += win[lv]->GetTotalDeltaSizeInDisk(from, to);
        }
        
        /* Get for versioned_ov_array */
        if((RandRDWR == WRONLY || RandRDWR == RDWR)) {
            bytes += versioned_ov_array->GetTotalDeltaSizeInDisk(from, to);
        }
        return bytes;
    }
    
    /**
     * Return size of vector element
     */
    virtual int64_t GetSizeOfVectorElement() {
        return sizeof(T);
    }
    
    /* Return ggb */
    virtual void* GetGGB() {
        return (void*) global_gather_buffer;
    }
    
    /* Return old message ggb */
    virtual void* GetOMGGB() {
        return (void*) OM_global_gather_buffer;
    }

    /* Get vector names*/
    virtual void getvectornames(std::vector<json11::Json>& vector_names) {
        vector_names.push_back(versioned_ov_array->GetArrayName());
        if (win[0] == NULL) return;
        /* Append names to the vector */
        win[0]->getvectornames(vector_names);
    }
    
    /* Write versioned array IO wait time for input/output vector */
    virtual void writeVersionedArrayIOWaitTime(JsonLogger*& json_logger) {
        versioned_ov_array->writeVersionedArrayIOWaitTime(json_logger);
        if (win[0] == NULL) return;
        win[0]->writeVersionedArrayIOWaitTime(json_logger);
    }
    
    /* Log versioned array IO wait time for input/output vector */
    virtual void logVersionedArrayIOWaitTime() {
        versioned_ov_array->logVersionedArrayIOWaitTime();
        if (win[0] == NULL) return;
        win[0]->logVersionedArrayIOWaitTime();
    }
    
    /* Aggregate read IO for input/output vector  */
    virtual void aggregateReadIO(JsonLogger*& json_logger) {
        versioned_ov_array->aggregateReadIO(json_logger);
        if (win[0] == NULL) return;
        win[0]->aggregateReadIO(json_logger);
    }
    
    /* Aggregate write IO for input/output vector  */
    virtual void aggregateWriteIO(JsonLogger*& json_logger) {
        versioned_ov_array->aggregateWriteIO(json_logger);
        if (win[0] == NULL) return;
        win[0]->aggregateWriteIO(json_logger);
    }
    
    /* Aggregate non-overlapped time for input/output vector  */
    virtual void aggregateNonOverlappedTime(JsonLogger*& json_logger) {
        versioned_ov_array->aggregateNonOverlappedTime(json_logger);
        if (win[0] == NULL) return;
        win[0]->aggregateNonOverlappedTime(json_logger);
    }
	
    T iden_elem;
        private:

	T* cur_read_random_vector_buff;
	T* fut_read_random_vector_buff;

	int64_t cur_src_vector_part;
	int64_t cur_src_vector_part_machine;
	int64_t cur_src_vector_part_chunk;

	int64_t cur_dst_vector_part;
	int64_t cur_dst_vector_part_machine;
	int64_t cur_dst_vector_part_chunk;

	Range<node_t> cur_src_vector_vid_range;
	Range<node_t> cur_dst_vector_vid_range;

	int64_t cur_rand_write_idx;

	node_t cur_dst_vector_first_vid;
	Range<node_t>* cur_dst_vector_part_writeback_tasks[2];
	std::future<void> write_rand_thread[2];
	std::future<void> read_rand_thread;

	//if in_memory
	T* seq_read_vector_vid_indexable;
	T* seq_read_vector;
	T* seq_write_vector_vid_indexable;
	T* seq_write_vector;

	//for cut vertex
	T* cut_read_vector;
	T* cut_write_vector;

	// Buffer for Seq.Write (or Receive buffer for Rand.Write) in Memory
	T* rand_write_buff_merged_vid_indexable;
	T* rand_write_buff_merged;
	T* rand_write_buff_being_merged;
	std::future<void> rand_write_merge_request;
	Range<node_t> rand_write_buff_merged_vid_range;

	MemoryMappedArray<T>* cur_ov_mmap;
    std::vector<MemoryMappedArray<T>*> ov_mmaps;
	
#ifdef GB_VERSIONED_ARRAY
    GBVersionedArrayBase<T, op>* versioned_ov_array;
    //VersionedArrayBase<T, op>* versioned_ov_array;
#else
    VersionedArrayBase<T, op>* versioned_ov_array;
#endif
    T* global_gather_buffer;
    T* OM_global_gather_buffer;
	Range<node_t> global_gather_buffer_vid_range;
    VECTOR<VECTOR<Turbo_bin_io_handler>> updates_spilling_handler;
	int64_t cur_chunk_merged;
	std::atomic<int64_t> spilled_msg_bytes;

	WritebackMessage<T> ** per_thread_write_vector;
	PaddedIdx* per_thread_idx;
	WritebackMessage<T> * small_message_to_send; // TODO not used
	int64_t small_message_idx;

	int thread_buff_overflow;
	int prev_thread_buff_overflow;

	T identity_element;
	MPI_Datatype datatype;

	int64_t rand_read_combined;
	int64_t rand_write_combined;

    //std::vector<TG_DistributedVectorWindow<T, op>*> win;
    // TODO the current implementation uses at most two windows per vector */
    /* Array of windows for the vector */
    TG_DistributedVectorWindow<T, op>* win[2];  // XXX - temporary fix

    bool skip_iv; /* Whether to skip reading input vector */
    bool construct_next_ss_array_async;
    bool pull_flag=false;
    int64_t pull_buffer_idx;

    int cur;

    // TODO need name change
    turbo_timer syko_timer;
    turbo_timer tslee_timer;

	CACHE_PADOUT;
} CACHE_ALIGNED;
    
template<> 
template<>
inline bool TG_DistributedVector<int64_t, MIN>::CheckOpRequiresReaggregationForDeletion<int64_t, MIN>(int64_t& agg_val) {
    if (((int64_t) GET_CNT_64BIT(agg_val)) <= 0) return true;
    else return false;
}
template<> 
template<>
inline bool TG_DistributedVector<int32_t, MIN>::CheckOpRequiresReaggregationForDeletion<int32_t, MIN>(int32_t& agg_val) {
    if (((int32_t) GET_CNT_32BIT(agg_val)) <= 0) return true;
    else return false;
}

template<>
template<>
inline bool TG_DistributedVector<LatentVector<int32_t, LATENT_FACTOR_K>, MIN>::CheckOpRequiresReaggregationForDeletion<LatentVector<int32_t, LATENT_FACTOR_K>, MIN>(LatentVector<int32_t, LATENT_FACTOR_K>& agg_val) {
		for (int i = 0; i < LATENT_FACTOR_K; i++) {
			if (((int32_t) GET_CNT_32BIT(agg_val[i])) <= 0) return true;
		}
		return false;
}

/**
 * Definition for accumulationg updates incrementally for each combination of aggergate function and datatype
 */
template<> template<>
inline int64_t TG_DistributedVector<int64_t, MIN>::AccumulatedUpdateInternal<int64_t, MIN>(node_t vid) {
    ALWAYS_ASSERT (RandRDWR == WRONLY || RandRDWR == RDWR);
    if (UserArguments::USE_DEGREE_ORDER_REPRESENTATION) {
        vid = PartitionStatistics::DegreeOrderToVid(vid);
    }
    ALWAYS_ASSERT (LocalStatistics::IsInternalVertex(vid));
    ALWAYS_ASSERT (rand_write_buff_merged_vid_range.contains(vid));
    return GET_VAL_64BIT(rand_write_buff_merged_vid_indexable[vid]);
}

template<> template<>
inline int64_t TG_DistributedVector<int64_t, MAX>::AccumulatedUpdateInternal<int64_t, MAX>(node_t vid) {
    ALWAYS_ASSERT (RandRDWR == WRONLY || RandRDWR == RDWR);
    if (UserArguments::USE_DEGREE_ORDER_REPRESENTATION) {
        vid = PartitionStatistics::DegreeOrderToVid(vid);
    }
    ALWAYS_ASSERT (LocalStatistics::IsInternalVertex(vid));
    ALWAYS_ASSERT (rand_write_buff_merged_vid_range.contains(vid));
    return GET_VAL_64BIT(rand_write_buff_merged_vid_indexable[vid]);
}

template<> template<>
inline int32_t TG_DistributedVector<int32_t, PLUS>::AccumulatedUpdateInternal<int32_t, PLUS>(node_t vid) {
    ALWAYS_ASSERT (RandRDWR == WRONLY || RandRDWR == RDWR);
    if (UserArguments::USE_DEGREE_ORDER_REPRESENTATION) {
        vid = PartitionStatistics::DegreeOrderToVid(vid);
    }
    ALWAYS_ASSERT (LocalStatistics::IsInternalVertex(vid));
    ALWAYS_ASSERT (rand_write_buff_merged_vid_range.contains(vid));
    return rand_write_buff_merged_vid_indexable[vid];
}

template<> template<>
inline int64_t TG_DistributedVector<int64_t, PLUS>::AccumulatedUpdateInternal<int64_t, PLUS>(node_t vid) {
    ALWAYS_ASSERT (RandRDWR == WRONLY || RandRDWR == RDWR);
    if (UserArguments::USE_DEGREE_ORDER_REPRESENTATION) {
        vid = PartitionStatistics::DegreeOrderToVid(vid);
    }
    ALWAYS_ASSERT (LocalStatistics::IsInternalVertex(vid));
    ALWAYS_ASSERT (rand_write_buff_merged_vid_range.contains(vid));
    return rand_write_buff_merged_vid_indexable[vid];
}

template<> template<>
inline float TG_DistributedVector<float, PLUS>::AccumulatedUpdateInternal<float, PLUS>(node_t vid) {
    ALWAYS_ASSERT (RandRDWR == WRONLY || RandRDWR == RDWR);
    if (UserArguments::USE_DEGREE_ORDER_REPRESENTATION) {
        vid = PartitionStatistics::DegreeOrderToVid(vid);
    }
    ALWAYS_ASSERT (LocalStatistics::IsInternalVertex(vid));
    ALWAYS_ASSERT (rand_write_buff_merged_vid_range.contains(vid));
    return rand_write_buff_merged_vid_indexable[vid];
}

template<> template<>
inline double TG_DistributedVector<double, PLUS>::AccumulatedUpdateInternal<double, PLUS>(node_t vid) {
    ALWAYS_ASSERT (RandRDWR == WRONLY || RandRDWR == RDWR);
    if (UserArguments::USE_DEGREE_ORDER_REPRESENTATION) {
        vid = PartitionStatistics::DegreeOrderToVid(vid);
    }
    ALWAYS_ASSERT (LocalStatistics::IsInternalVertex(vid));
    ALWAYS_ASSERT (rand_write_buff_merged_vid_range.contains(vid));
    return rand_write_buff_merged_vid_indexable[vid];
}

template<> template<>
inline bool TG_DistributedVector<bool, LOR>::AccumulatedUpdateInternal<bool, LOR>(node_t vid) {
    ALWAYS_ASSERT (RandRDWR == WRONLY || RandRDWR == RDWR);
    if (UserArguments::USE_DEGREE_ORDER_REPRESENTATION) {
        vid = PartitionStatistics::DegreeOrderToVid(vid);
    }
    ALWAYS_ASSERT (LocalStatistics::IsInternalVertex(vid));
    ALWAYS_ASSERT (rand_write_buff_merged_vid_range.contains(vid));
    return rand_write_buff_merged_vid_indexable[vid];
}

template<> template<>
inline LatentVector<int32_t, LATENT_FACTOR_K> TG_DistributedVector<LatentVector<int32_t, LATENT_FACTOR_K>, PLUS>::AccumulatedUpdateInternal<LatentVector<int32_t, LATENT_FACTOR_K>, PLUS>(node_t vid) {
    ALWAYS_ASSERT (RandRDWR == WRONLY || RandRDWR == RDWR);
    if (UserArguments::USE_DEGREE_ORDER_REPRESENTATION) {
        vid = PartitionStatistics::DegreeOrderToVid(vid);
    }
    ALWAYS_ASSERT (LocalStatistics::IsInternalVertex(vid));
    ALWAYS_ASSERT (rand_write_buff_merged_vid_range.contains(vid));
    return rand_write_buff_merged_vid_indexable[vid];
}

template<> template<>
inline LatentVector<int64_t, LATENT_FACTOR_K> TG_DistributedVector<LatentVector<int64_t, LATENT_FACTOR_K>, PLUS>::AccumulatedUpdateInternal<LatentVector<int64_t, LATENT_FACTOR_K>, PLUS>(node_t vid) {
    ALWAYS_ASSERT (RandRDWR == WRONLY || RandRDWR == RDWR);
    if (UserArguments::USE_DEGREE_ORDER_REPRESENTATION) {
        vid = PartitionStatistics::DegreeOrderToVid(vid);
    }
    ALWAYS_ASSERT (LocalStatistics::IsInternalVertex(vid));
    ALWAYS_ASSERT (rand_write_buff_merged_vid_range.contains(vid));
    return rand_write_buff_merged_vid_indexable[vid];
}

template<> template<>
inline LatentVector<float, LATENT_FACTOR_K> TG_DistributedVector<LatentVector<float, LATENT_FACTOR_K>, PLUS>::AccumulatedUpdateInternal<LatentVector<float, LATENT_FACTOR_K>, PLUS>(node_t vid) {
    ALWAYS_ASSERT (RandRDWR == WRONLY || RandRDWR == RDWR);
    if (UserArguments::USE_DEGREE_ORDER_REPRESENTATION) {
        vid = PartitionStatistics::DegreeOrderToVid(vid);
    }
    ALWAYS_ASSERT (LocalStatistics::IsInternalVertex(vid));
    ALWAYS_ASSERT (rand_write_buff_merged_vid_range.contains(vid));
    return rand_write_buff_merged_vid_indexable[vid];
}

template<> template<>
inline LatentVector<double, LATENT_FACTOR_K> TG_DistributedVector<LatentVector<double, LATENT_FACTOR_K>, PLUS>::AccumulatedUpdateInternal<LatentVector<double, LATENT_FACTOR_K>, PLUS>(node_t vid) {
    ALWAYS_ASSERT (RandRDWR == WRONLY || RandRDWR == RDWR);
    if (UserArguments::USE_DEGREE_ORDER_REPRESENTATION) {
        vid = PartitionStatistics::DegreeOrderToVid(vid);
    }
    ALWAYS_ASSERT (LocalStatistics::IsInternalVertex(vid));
    ALWAYS_ASSERT (rand_write_buff_merged_vid_range.contains(vid));
    return rand_write_buff_merged_vid_indexable[vid];
}

template<> template<>
inline LatentVector<int32_t, LATENT_FACTOR_K> TG_DistributedVector<LatentVector<int32_t, LATENT_FACTOR_K>, MIN>::AccumulatedUpdateInternal<LatentVector<int32_t, LATENT_FACTOR_K>, MIN>(node_t vid) {
		LatentVector<int32_t, LATENT_FACTOR_K> ret_val;
    ALWAYS_ASSERT (RandRDWR == WRONLY || RandRDWR == RDWR);
    if (UserArguments::USE_DEGREE_ORDER_REPRESENTATION) {
        vid = PartitionStatistics::DegreeOrderToVid(vid);
    }
    ALWAYS_ASSERT (LocalStatistics::IsInternalVertex(vid));
    ALWAYS_ASSERT (rand_write_buff_merged_vid_range.contains(vid));
		for (int i = 0; i < LATENT_FACTOR_K; i++) {
			ret_val[i] = GET_VAL_32BIT(rand_write_buff_merged_vid_indexable[vid][i]);
		}
    return ret_val;
}

template<> template<>
inline LatentVector<int64_t, LATENT_FACTOR_K> TG_DistributedVector<LatentVector<int64_t, LATENT_FACTOR_K>, MIN>::AccumulatedUpdateInternal<LatentVector<int64_t, LATENT_FACTOR_K>, MIN>(node_t vid) {
		LatentVector<int64_t, LATENT_FACTOR_K> ret_val;
    ALWAYS_ASSERT (RandRDWR == WRONLY || RandRDWR == RDWR);
    if (UserArguments::USE_DEGREE_ORDER_REPRESENTATION) {
        vid = PartitionStatistics::DegreeOrderToVid(vid);
    }
    ALWAYS_ASSERT (LocalStatistics::IsInternalVertex(vid));
    ALWAYS_ASSERT (rand_write_buff_merged_vid_range.contains(vid));
		for (int i = 0; i < LATENT_FACTOR_K; i++) {
			ret_val[i] = GET_VAL_64BIT(rand_write_buff_merged_vid_indexable[vid][i]);
		}
		return ret_val;
}

/**
 * Update vid for value
 */
template <> template <>
inline void TG_DistributedVector<int64_t, PLUS>::Update<0>(node_t vid, int64_t value, int64_t old_value) {
    ALWAYS_ASSERT (RandRDWR == WRONLY || RandRDWR == RDWR);
    if (UserArguments::USE_DEGREE_ORDER_REPRESENTATION) {
        vid = PartitionStatistics::DegreeOrderToVid(vid);
    }
#ifdef OldMessageTransfer
    WritebackMessageWOOldMessage<int64_t> msg;
    msg.dst_vid = vid;
    msg.message = idenelem<int64_t>(PLUS);
    UpdateAggregation<int64_t, PLUS>(&msg.message, old_value, value);
#else
    LOG_ASSERT(false);
    WritebackMessage<int64_t> msg;
#endif
    _AccumulateMessageIntoGGBInLocal(msg);
}

/**
 * Receive output vector message push for incremental processing
 */
template<typename T, Op op>
void TG_DistributedVector<T, op>::RespondOutputVectorMessagePushForIncProcessing(
    long fromChunkID, //int64_t fromChunkID, 
    long chunkID, //int64_t chunkID, 
    int partition_id, 
    int tid, 
    int idx, 
    long send_num, //int64_t send_num,
    long combined, //int64_t combined, 
    int lv) {
    INVARIANT(UserArguments::INCREMENTAL_PROCESSING);
    server_sockets.recv_from_client_lock(partition_id); // XXX
    while (TG_NWSM_Base::read_ggb_flag.load() == 1L) {
        _mm_pause();
    }
    //system_fprintf(0, stdout, "[%ld] (%d,%d,%d) RespondOutputVectorMessagePushForIncProcessing par_id = %d\n", PartitionStatistics::my_machine_id(), UserArguments::UPDATE_VERSION, UserArguments::SUPERSTEP, UserArguments::INC_STEP, partition_id);

    // TODO - wait for ReadGGB() to be completed here?

    turbo_timer tim;

    tim.start_timer(0);
    tim.start_timer(1);

    ALWAYS_ASSERT(chunkID < UserArguments::VECTOR_PARTITIONS);

    Range<node_t> cur_chunk_vid_range = PartitionStatistics::my_chunkID_to_range(chunkID);
#ifdef OldMessageTransfer
    int64_t size_once = DEFAULT_NIO_BUFFER_SIZE / sizeof(WritebackMessageWOOldMessage<T>);
    WritebackMessageWOOldMessage<T>* recv_buff_ = new WritebackMessageWOOldMessage<T>[size_once];
#else
    int64_t size_once = DEFAULT_NIO_BUFFER_SIZE / sizeof(WritebackMessage<T>);
    WritebackMessage<T>* recv_buff_ = new WritebackMessage<T>[size_once];       // TODO - remove dynamic memory allocation
#endif
    ALWAYS_ASSERT (recv_buff_ != nullptr);

    int mpi_error = -1;
    int64_t recv_bytes;
    MPI_Status stat;
    int64_t accm_recv_bytes = 0;

    bool run = true;
    tim.stop_timer(1);
    
    /** Repeat while receive finishes */
    while (run) {
        /** Receive data */
        tim.start_timer(2);
        recv_bytes = server_sockets.recv_from_client((char*) recv_buff_, 0, partition_id);
        tim.stop_timer(2);
        tim.start_timer(3);
#ifdef OldMessageTransfer
        ALWAYS_ASSERT (recv_bytes <= size_once * sizeof(WritebackMessageWOOldMessage<T>));

        int64_t entry_to_recv = recv_bytes / sizeof(WritebackMessageWOOldMessage<T>);
#else
        ALWAYS_ASSERT (recv_bytes <= size_once * sizeof(WritebackMessage<T>));

        int64_t entry_to_recv = recv_bytes / sizeof(WritebackMessage<T>);
#endif
        accm_recv_bytes += recv_bytes;

        /** Check overflow */
        int64_t temp_idx = TG_DistributedVectorBase::write_vec_changed_idx.fetch_add(entry_to_recv);
        if(temp_idx + entry_to_recv >= VECTOR_UPDATE_AGGREGATION_THRESHOLD) {
            TG_DistributedVectorBase::update_delta_buffer_overflow.store(true);
        }
        tim.stop_timer(3);

        /** Check if delta update buffer overflows */
        int64_t to_be_spilled = 0;
        if(TG_DistributedVectorBase::update_delta_buffer_overflow.load()) {
            tim.start_timer(4);
            int64_t i = 0;
            for(i = 0 ; i < entry_to_recv; i++) {
                ALWAYS_ASSERT (cur_chunk_vid_range.contains(recv_buff_[i].dst_vid));
                if (!global_gather_buffer_vid_range.contains(recv_buff_[i].dst_vid)) {
                    recv_buff_[to_be_spilled] = recv_buff_[i];
                    to_be_spilled++;
                } else {
                    _AccumulateMessageIntoGGB(recv_buff_[i], partition_id);
                }
            }
            tim.stop_timer(4);
        } else {
            tim.start_timer(5);
            int64_t i = 0;
            for(i = 0 ; i < entry_to_recv; i++) {
                ALWAYS_ASSERT (cur_chunk_vid_range.contains(recv_buff_[i].dst_vid));
                if (!global_gather_buffer_vid_range.contains(recv_buff_[i].dst_vid)) {
                    recv_buff_[to_be_spilled] = recv_buff_[i];
                    to_be_spilled++;
                } else {
                    _AccumulateMessageIntoGGB(recv_buff_[i], partition_id);
                }
                TG_DistributedVectorBase::write_vec_changed[temp_idx] = recv_buff_[i].dst_vid;
                temp_idx++;
            }
            tim.stop_timer(5);
        }

        // Spill the data [i, entry_to_recv) in recv_buff_ into disks
        // It's guaranteed that the rest of the messages in 'recv_buff_[i, -)' cannot be accumulated in memory
        tim.start_timer(6);
        if (to_be_spilled > 0) {
            int64_t to_chunk_id = chunkID;
            int64_t from_machine_id = partition_id;
            int64_t from_chunk_id = fromChunkID;
#ifdef OldMessageTransfer
            int64_t size_to_append = sizeof(WritebackMessageWOOldMessage<T>) * to_be_spilled;
#else
            int64_t size_to_append = sizeof(WritebackMessage<T>) * to_be_spilled;
#endif
            SpillVectorWriteIntoDisk((char*) &recv_buff_[0], size_to_append, from_machine_id, from_chunk_id, to_chunk_id);
        }
        tim.stop_timer(6);

        if (recv_bytes == 0) {
            run = false;
        }
    }
    tim.start_timer(7);
    delete[] recv_buff_;
    rand_write_recv_eom_count+=combined;
    tim.stop_timer(7);

    tim.start_timer(8);
    server_sockets.recv_from_client_unlock(partition_id);
    double elapsed_time = tim.stop_timer(0);
    tim.stop_timer(8);

    //fprintf(stdout, "[%ld][%s] RespondOV Superstep (%ld, %ld) %ld [MB] ReceiveOV %.4f %.4f %.4f %.4f %.4f %.4f %.4f %.4f %.4f\n", PartitionStatistics::my_machine_id(), this->vector_name.c_str(),(int64_t) UserArguments::UPDATE_VERSION, UserArguments::SUPERSTEP, accm_recv_bytes, tim.get_timer(0), tim.get_timer(1), tim.get_timer(2), tim.get_timer(3), tim.get_timer(4), tim.get_timer(5), tim.get_timer(6), tim.get_timer(7), tim.get_timer(8));
    //fprintf(stdout, "[%ld][%s] RespondOV Superstep (%ld, %ld) %.3f [MB] ReceiveOV %.4f %.4f %.4f %.4f %.4f %.4f\n", PartitionStatistics::my_machine_id(), this->vector_name.c_str(),(int64_t) UserArguments::UPDATE_VERSION, UserArguments::SUPERSTEP, (double) accm_recv_bytes / (1024*1024), tim.get_timer(0), tim.get_timer(1), tim.get_timer(2), tim.get_timer(3), tim.get_timer(4), tim.get_timer(5));
#ifdef REPORT_PROFILING_TIMERS
    //fprintf(stdout, "[%s] RespondOV Superstep (%ld, %ld) %.3f [MB] ReceiveOV %ld %.4f %.4f\n", this->vector_name.c_str(),(int64_t) UserArguments::UPDATE_VERSION, UserArguments::SUPERSTEP, PartitionStatistics::my_machine_id(), (double) accm_recv_bytes / (1024*1024), tim.get_timer(0), tim.get_timer(1));
#endif
}

/**
 * Receive output vector message push for static processing
 */
template<typename T, Op op>
void TG_DistributedVector<T, op>::RespondOutputVectorMessagePushForStaticProcessing(int64_t fromChunkID, int64_t chunkID, int partition_id, int tid, int idx, int64_t send_num, int64_t combined, int lv) {
    ALWAYS_ASSERT(TG_DistributedVectorBase::write_vec_changed != nullptr);
    server_sockets.recv_from_client_lock(partition_id); // XXX
    //fprintf(stdout, "[%ld][step:%d] RespondOutputVectorMessagePushForStaticProcessing vector %s from %d\n", PartitionStatistics::my_machine_id(), UserArguments::INC_STEP, this->vector_name.c_str(), partition_id);
    if (UserArguments::INC_STEP == 3) {
        TG_DistributedVectorBase::ggb_pull_idx_per_machine_idx[partition_id].idx = 0;
        TG_DistributedVectorBase::ggb_pull_idx_overflow[partition_id] = false;
    }
    while (TG_NWSM_Base::read_ggb_flag.load() == 1L) {
        _mm_pause();
    }

    turbo_timer tim;
    tim.start_timer(0);

    Range<node_t> cur_chunk_vid_range = PartitionStatistics::my_chunkID_to_range(chunkID);
#ifdef OldMessageTransfer
    int64_t size_once = DEFAULT_NIO_BUFFER_SIZE / sizeof(WritebackMessageWOOldMessage<T>);
    WritebackMessageWOOldMessage<T>* recv_buff_ = new WritebackMessageWOOldMessage<T>[size_once];
#else
    int64_t size_once = DEFAULT_NIO_BUFFER_SIZE / sizeof(WritebackMessage<T>);
    WritebackMessage<T>* recv_buff_ = new WritebackMessage<T>[size_once];
#endif


    int mpi_error = -1;
    int recv_bytes;
    MPI_Status stat;
    int64_t accm_recv_bytes = 0;
    int64_t accm_idx_changes = 0;

    /** Repeat while receive finishes */
    bool run = true;
    while (run) {
        tim.start_timer(1);
        /** Receive data and print statistics */
#ifdef OldMessageTransfer
        recv_bytes = server_sockets.recv_from_client((char*) recv_buff_, 0, partition_id, size_once * sizeof(WritebackMessageWOOldMessage<T>), true);
        tim.stop_timer(1);
        if (recv_bytes > size_once * sizeof(WritebackMessageWOOldMessage<T>)) {
            fprintf(stdout, "[%ld] from %ld RespondOutputVectorMessagePushForStaticProcessing vector %s error recv_bytes = %ld, sizeof = %ld\n", PartitionStatistics::my_machine_id(), partition_id, this->vector_name.c_str(), recv_bytes, size_once * sizeof(WritebackMessageWOOldMessage<T>));
        }
        ALWAYS_ASSERT (recv_bytes <= size_once * sizeof(WritebackMessageWOOldMessage<T>));

        if (recv_bytes % sizeof(WritebackMessageWOOldMessage<T>) != 0) {
            fprintf(stdout, "[%ld] from %ld RespondOutputVectorMessagePushForStaticProcessing vector %s error recv_bytes = %ld, sizeof = %ld\n", PartitionStatistics::my_machine_id(), partition_id, this->vector_name.c_str(), recv_bytes, sizeof(WritebackMessageWOOldMessage<T>));
            continue;
        }
        INVARIANT(recv_bytes % sizeof(WritebackMessageWOOldMessage<T>) == 0); //XXX how to handle?
        int64_t entry_to_recv = recv_bytes / sizeof(WritebackMessageWOOldMessage<T>);
#else
        recv_bytes = server_sockets.recv_from_client((char*) recv_buff_, 0, partition_id, size_once * sizeof(WritebackMessage<T>), true);
        tim.stop_timer(1);
        if (recv_bytes > size_once * sizeof(WritebackMessage<T>)) {
            fprintf(stdout, "[%ld] from %ld RespondOutputVectorMessagePushForStaticProcessing vector %s error recv_bytes = %ld, sizeof = %ld\n", PartitionStatistics::my_machine_id(), partition_id, this->vector_name.c_str(), recv_bytes, size_once * sizeof(WritebackMessage<T>));
        }
        ALWAYS_ASSERT (recv_bytes <= size_once * sizeof(WritebackMessage<T>));

        if (recv_bytes % sizeof(WritebackMessage<T>) != 0) {
            fprintf(stdout, "[%ld] from %ld RespondOutputVectorMessagePushForStaticProcessing vector %s error recv_bytes = %ld, sizeof = %ld\n", PartitionStatistics::my_machine_id(), partition_id, this->vector_name.c_str(), recv_bytes, sizeof(WritebackMessage<T>));
            continue;
        }
        INVARIANT(recv_bytes % sizeof(WritebackMessage<T>) == 0); //XXX how to handle?
        int64_t entry_to_recv = recv_bytes / sizeof(WritebackMessage<T>);
#endif
        accm_recv_bytes += recv_bytes;
        accm_idx_changes += entry_to_recv;
        int64_t temp_idx = TG_DistributedVectorBase::write_vec_changed_idx.fetch_add(entry_to_recv);

        /** Check overflow */
        if(temp_idx + entry_to_recv >= VECTOR_UPDATE_AGGREGATION_THRESHOLD) {
            TG_DistributedVectorBase::update_delta_buffer_overflow.store(true);
        }

        /** Check if delta update buffer overflows */
        int64_t to_be_spilled = 0;
        if(TG_DistributedVectorBase::update_delta_buffer_overflow.load()) {
            int64_t i = 0;
            for(i = 0 ; i < entry_to_recv; i++) {
                ALWAYS_ASSERT (cur_chunk_vid_range.contains(recv_buff_[i].dst_vid));
                if (!global_gather_buffer_vid_range.contains(recv_buff_[i].dst_vid)) {
                    recv_buff_[to_be_spilled] = recv_buff_[i];
                    to_be_spilled++;
                } else {
                    //int64_t loc = recv_buff_[i].dst_vid - global_gather_buffer_vid_range.GetBegin();
                    //ALWAYS_ASSERT (loc >= 0 && loc < global_gather_buffer_vid_range.length());
                    _AccumulateMessageIntoGGB(recv_buff_[i], partition_id);
                }
            }
        } else {
            int64_t i = 0;
            int64_t to_be_spilled = 0;
            for(i = 0 ; i < entry_to_recv; i++) {
                ALWAYS_ASSERT (cur_chunk_vid_range.contains(recv_buff_[i].dst_vid));
                if (!global_gather_buffer_vid_range.contains(recv_buff_[i].dst_vid)) {
                    recv_buff_[to_be_spilled] = recv_buff_[i];
                    to_be_spilled++;
                } else {
                    //int64_t loc = recv_buff_[i].dst_vid - global_gather_buffer_vid_range.GetBegin();
                    //ALWAYS_ASSERT (loc >= 0 && loc < global_gather_buffer_vid_range.length());
                    _AccumulateMessageIntoGGB(recv_buff_[i], partition_id);
                }
                TG_DistributedVectorBase::write_vec_changed[temp_idx] = recv_buff_[i].dst_vid;
                temp_idx++;
            }
        }
        // Spill the data [i, entry_to_recv) in recv_buff into disks
        // It's guaranteed that the rest of the messages in 'recv_buff[i, -)' cannot be accumulated in memory
        if (to_be_spilled > 0) {
            abort(); // XXX disabled by syko 2019.02.28
            /*int64_t to_chunk_id = chunkID;
                int64_t from_machine_id = partition_id;
                int64_t from_chunk_id = fromChunkID;
                int64_t size_to_append = sizeof(WritebackMessage<T>) * to_be_spilled;
                SpillVectorWriteIntoDisk((char*) &recv_buff_[0], size_to_append, from_machine_id, from_chunk_id, to_chunk_id);*/
        }

        if (recv_bytes == 0) {
            run = false;
        }
    }
    delete[] recv_buff_;
    rand_write_recv_eom_count+=combined;

    server_sockets.recv_from_client_unlock(partition_id);
    double elapsed_time = tim.stop_timer(0);

#ifdef REPORT_PROFILING_TIMERS
    fprintf(stdout, "[%s] RespondOV Superstep (%ld, %ld) %.3f [MB] ReceiveOV %ld %.4f %.4f\n", this->vector_name.c_str(),(int64_t) UserArguments::UPDATE_VERSION, UserArguments::SUPERSTEP, PartitionStatistics::my_machine_id(), (double) accm_recv_bytes / (1024*1024), tim.get_timer(0), tim.get_timer(1));
#endif
}

/**
 * Initialize output vector for given partition info
 */
template<typename T, Op op>
bool TG_DistributedVector<T, op>::InitializeOutputVector(int64_t start_part_id, int64_t part_id, int64_t next_part_id) {
    if (win[UserArguments::CURRENT_LEVEL] == NULL) {
        return false;
    }

    /* Update current output vector status */
    cur_dst_vector_part = part_id;
    cur_dst_vector_part_machine = part_id / UserArguments::VECTOR_PARTITIONS;
    cur_dst_vector_part_chunk = part_id % UserArguments::VECTOR_PARTITIONS;
    cur_dst_vector_first_vid = PartitionStatistics::machine_id_and_chunk_idx_to_vid_range(cur_dst_vector_part_machine, cur_dst_vector_part_chunk).GetBegin();
    cur_dst_vector_vid_range = PartitionStatistics::machine_id_and_chunk_idx_to_vid_range(cur_dst_vector_part_machine, cur_dst_vector_part_chunk);

    int machine_id = part_id / UserArguments::VECTOR_PARTITIONS;
    int64_t chunk_id = part_id % UserArguments::VECTOR_PARTITIONS;
    int next_machine_id = next_part_id / UserArguments::VECTOR_PARTITIONS;
    int64_t next_chunk_id = next_part_id % UserArguments::VECTOR_PARTITIONS;
    int64_t idx_to_wait = cur;
    int64_t idx_to_put_future = (cur + 1) % 2;

    // TODO not using this variable
    cur_rand_write_idx = idx_to_put_future;

    /* Call Initialize output vector for current level of window */
    win[UserArguments::CURRENT_LEVEL]->InitializeOutputVector(start_part_id, part_id, next_part_id);
    return true;
}



#endif
